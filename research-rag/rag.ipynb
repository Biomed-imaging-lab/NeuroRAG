{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import transformers\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.chains.question_answering.stuff_prompt import CHAT_PROMPT as DEFAULT_PROMPT\n",
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['MISTRAL_API_KEY'] = os.getenv('MISTRAL_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/vladimirskvortsov/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "  'cuda'\n",
    "  if torch.cuda.is_available()\n",
    "  else 'mps'\n",
    "  if torch.backends.mps.is_available()\n",
    "  else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(corpus):\n",
    "  corpus = corpus.lower()\n",
    "  stopset = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('russian') + list(string.punctuation)\n",
    "  tokens = nltk.word_tokenize(corpus)\n",
    "  tokens = [t for t in tokens if t not in stopset]\n",
    "  tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "  corpus = ' '.join(tokens)\n",
    "  corpus = unidecode(corpus)\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_cosine_sim_metric(expected_answers, predicted_answers):\n",
    "  results = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    expected_embedding = np.array(embeddings.embed_query(expected_answer))\n",
    "    predicted_embedding = np.array(embeddings.embed_query(predicted_answer))\n",
    "\n",
    "    sim = cosine_similarity(\n",
    "      expected_embedding.reshape(1, -1),\n",
    "      predicted_embedding.reshape(1, -1),\n",
    "    )[0][0]\n",
    "\n",
    "    results.append(sim)\n",
    "\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    predicted_tokens = nltk.word_tokenize(predicted_answer)\n",
    "    expected_tokens = [nltk.word_tokenize(expected_answer)]\n",
    "\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu_score = sentence_bleu(expected_tokens, predicted_tokens, smoothing_function=smoothie)\n",
    "\n",
    "    scores.append(bleu_score)\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the afferent cranial nerve nuclei?</td>\n",
       "      <td>Trigeminal sensory nucleus- fibres carry gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the order of the cranial nerves ?</td>\n",
       "      <td>1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the efferent cranial nerve nuclei?</td>\n",
       "      <td>Edinger-westphal nucleus\\nOculomotor nucleus\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which nuclei share the embryo logical origin -...</td>\n",
       "      <td>Oculomotor nucleus Trochlear nucleus Abducens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which nuclei share the embryo logical origin- ...</td>\n",
       "      <td>Trigeminal motor nucleus Facial motor nucleus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>What is the purpose of gephyrin in the glycine...</td>\n",
       "      <td>Involved in anchoring the receptor to a specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>What is the glycine receptor involved in ?</td>\n",
       "      <td>Reflex response\\nCauses reciprocal inhibition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>What happens in hyperperplexia ?</td>\n",
       "      <td>It’s an exaggerated reflex Often caused by a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>What is hyperperplexia treated with ?</td>\n",
       "      <td>Benzodiazepine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>What increases glycine release ?</td>\n",
       "      <td>Tetanus toxin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1052 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0           What are the afferent cranial nerve nuclei?   \n",
       "1             What is the order of the cranial nerves ?   \n",
       "2           What are the efferent cranial nerve nuclei?   \n",
       "3     Which nuclei share the embryo logical origin -...   \n",
       "4     Which nuclei share the embryo logical origin- ...   \n",
       "...                                                 ...   \n",
       "1047  What is the purpose of gephyrin in the glycine...   \n",
       "1048         What is the glycine receptor involved in ?   \n",
       "1049                   What happens in hyperperplexia ?   \n",
       "1050              What is hyperperplexia treated with ?   \n",
       "1051                   What increases glycine release ?   \n",
       "\n",
       "                                                 answer  \n",
       "0     Trigeminal sensory nucleus- fibres carry gener...  \n",
       "1     1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...  \n",
       "2     Edinger-westphal nucleus\\nOculomotor nucleus\\n...  \n",
       "3     Oculomotor nucleus Trochlear nucleus Abducens ...  \n",
       "4     Trigeminal motor nucleus Facial motor nucleus ...  \n",
       "...                                                 ...  \n",
       "1047  Involved in anchoring the receptor to a specif...  \n",
       "1048  Reflex response\\nCauses reciprocal inhibition ...  \n",
       "1049  It’s an exaggerated reflex Often caused by a m...  \n",
       "1050                                     Benzodiazepine  \n",
       "1051                                      Tetanus toxin  \n",
       "\n",
       "[1052 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.read_csv('../research-neurobiology-qa-dataset/brainscape.csv')\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = Path('./docs')\n",
    "docs = []\n",
    "\n",
    "for file in tqdm(docs_dir.iterdir()):\n",
    "  if file.is_file() and file.suffix == '.pdf':\n",
    "    loader = PDFMinerLoader(file)\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=700,\n",
    "  chunk_overlap=0,\n",
    "  length_function=len,\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama2_llm():\n",
    "  return Ollama(temperature=0, model='llama2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama3_llm():\n",
    "  return Ollama(temperature=0, model='llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openbiollm_parser(output):\n",
    "  idx = output.find('Helpful Answer: ')\n",
    "  if idx != -1:\n",
    "    return output[idx + len('Helpful answer: '):]\n",
    "  else:\n",
    "    return output\n",
    "\n",
    "def get_openbiollm_8b_llm():\n",
    "  model = 'aaditya/OpenBioLLM-Llama3-8B'\n",
    "  model_kwargs = {'torch_dtype': torch.bfloat16}\n",
    "  pipeline = transformers.pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    device=device,\n",
    "  )\n",
    "  terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids('<|eot_id|>')\n",
    "  ]\n",
    "  llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model,\n",
    "    task='text-generation',\n",
    "    model_kwargs=model_kwargs,\n",
    "    pipeline_kwargs={\n",
    "      'max_new_tokens': 256,\n",
    "      'eos_token_id': terminators,\n",
    "      'do_sample': True,\n",
    "      'temperature': 0.0001,\n",
    "      'top_p': 0.9,\n",
    "    },\n",
    "  )\n",
    "  return llm | openbiollm_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mistral_llm():\n",
    "  return ChatMistralAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup index stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_array_vector_store(docs=[]):\n",
    "  index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    "  ).from_documents(docs)\n",
    "  return index.vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_vector_store(docs=[]):\n",
    "  vector_store = Chroma.from_documents(docs, embeddings)\n",
    "  return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "few_shot_examples = [\n",
    "  {\n",
    "    \"question\": \"Which cranial nerves are motor?\",\n",
    "    \"answer\": \"Oculomotor\\nTrochlear \\nAbducens\\nAccessory\\nHypoglossal\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"ich of the cranial nerves have both sensory and motor control ?\",\n",
    "    \"answer\": \"TrigeminalFacial GlossopharyngealVagus\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Which regions of the cross section of the spinal cord have a larger ventral horn ?\",\n",
    "    \"answer\": \"The cervical and lumbar regions have larger ventral horns. The thoracic region has a smaller ventral horn region because it controls the trunk so not many motor neurones are coming out. Thoracic region has a more prominent lateral horn where preganglionic neurones are present\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What are the subdivisions of the vertebral column ?\",\n",
    "    \"answer\": \"Cervical = 8\\nThoracic= 12\\nLumbar=5 \\nSacral=5 \\nCoccygeal\"\n",
    "  },\n",
    "]\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    (\"human\", example_prompt_template),\n",
    "    (\"ai\", \"{answer}\\n\"),\n",
    "  ],\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "  example_prompt=example_prompt,\n",
    "  examples=few_shot_examples,\n",
    "  input_variables=[\"question\"],\n",
    ")\n",
    "base_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "You answer in very short sentences and do not include extra information.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\n",
    "\"\"\")\n",
    "final_few_shot_prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    few_shot_prompt,\n",
    "    base_prompt\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\n",
    "  ('LLaMA 2', get_llama2_llm()),\n",
    "  ('LLaMA 3', get_llama3_llm()),\n",
    "  ('OpenBioLLM Llama3 8B', get_openbiollm_8b_llm()),\n",
    "  ('Mistral', get_mistral_llm()),\n",
    "]\n",
    "\n",
    "vector_stores = [\n",
    "  ('DocArray', get_doc_array_vector_store),\n",
    "  ('Chroma', get_chroma_vector_store),\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "  ('Default', DEFAULT_PROMPT),\n",
    "  ('Few-shot prompting', final_few_shot_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['LLaMA 3_Doc Array In Memory Search_True', 'LLaMA 3_Doc Array In Memory Search_False', 'OpenBioLLM Llama3 8B_Doc Array In Memory Search_True', 'LLaMA 2_Doc Array In Memory Search_False', 'LLaMA 2_Doc Array In Memory Search_True', 'OpenBioLLM Llama3 8B_Doc Array In Memory Search_False'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_path = Path('cache.json')\n",
    "with open(cache_path, 'r') as f:\n",
    "  cache = json.load(f)\n",
    "\n",
    "cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLMs:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "LLMs: 100%|██████████| 2/2 [00:00<00:00, 71.21it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "sample_df = qa_df.sample(frac=1)\n",
    "questions = sample_df['question'].tolist()\n",
    "expected_answers = sample_df['answer'].tolist()\n",
    "\n",
    "for llm_name, llm in tqdm(llms, desc='LLMs'):\n",
    "  for vector_store_name, get_vector_store in tqdm(vector_stores, desc='Vector Stores', leave=False):\n",
    "    for use_docs in tqdm((False, True), desc='Use Docs', leave=False):\n",
    "      for prompt_name, prompt_template in tqdm(prompts, desc='Prompts', leave=False):\n",
    "        if use_docs == False and vector_store_name != 'DocArray':\n",
    "          continue\n",
    "        vector_store = get_vector_store(docs)\n",
    "        qa_llm = RetrievalQA.from_chain_type(\n",
    "          llm=llm,\n",
    "          chain_type='stuff',\n",
    "          retriever=vector_store.as_retriever(search_kwargs={\"k\" : 10}),\n",
    "          verbose=False,\n",
    "          chain_type_kwargs = {\n",
    "            'prompt': prompt_template,\n",
    "            'document_separator': '<<<<<>>>>>'\n",
    "          },\n",
    "        )\n",
    "\n",
    "        predicted_answers = []\n",
    "\n",
    "        for index, question in tqdm(enumerate(questions), desc='Questions', leave=False):\n",
    "          key = f'{llm_name}_{vector_store_name}_{use_docs}'\n",
    "\n",
    "          if not key in cache:\n",
    "            cache[key] = {}\n",
    "          if not question in cache[key]:\n",
    "            cache[key][question] = qa_llm.invoke(question)['result']\n",
    "\n",
    "          predicted_answers.append(cache[key][question])\n",
    "\n",
    "          with open(cache_path, 'w') as f:\n",
    "            json.dump(cache, f)\n",
    "\n",
    "        cos_sim = embeddings_cosine_sim_metric(expected_answers, predicted_answers)\n",
    "        bleu_score = bleu_metric(expected_answers, predicted_answers)\n",
    "\n",
    "        row = pd.DataFrame({\n",
    "          'llm': llm_name,\n",
    "          'vector_store': vector_store_name,\n",
    "          'use_docs': use_docs,\n",
    "          'prompt': prompt_name,\n",
    "          'cos_sim': cos_sim,\n",
    "          'bleu': bleu_score,\n",
    "        }, index=[0])\n",
    "        df = pd.concat([df, row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm</th>\n",
       "      <th>vector_store</th>\n",
       "      <th>use_docs</th>\n",
       "      <th>prompt</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>False</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>False</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>True</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>True</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>False</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>False</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>True</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLaMA 2</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>True</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>False</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>False</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>True</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>DocArray</td>\n",
       "      <td>True</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>False</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>False</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>True</td>\n",
       "      <td>Default</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LLaMA 3</td>\n",
       "      <td>Chroma</td>\n",
       "      <td>True</td>\n",
       "      <td>Few-shot prompting</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        llm vector_store  use_docs              prompt  cos_sim  bleu\n",
       "0   LLaMA 2     DocArray     False             Default        0     0\n",
       "1   LLaMA 2     DocArray     False  Few-shot prompting        0     0\n",
       "2   LLaMA 2     DocArray      True             Default        0     0\n",
       "3   LLaMA 2     DocArray      True  Few-shot prompting        0     0\n",
       "4   LLaMA 2       Chroma     False             Default        0     0\n",
       "5   LLaMA 2       Chroma     False  Few-shot prompting        0     0\n",
       "6   LLaMA 2       Chroma      True             Default        0     0\n",
       "7   LLaMA 2       Chroma      True  Few-shot prompting        0     0\n",
       "8   LLaMA 3     DocArray     False             Default        0     0\n",
       "9   LLaMA 3     DocArray     False  Few-shot prompting        0     0\n",
       "10  LLaMA 3     DocArray      True             Default        0     0\n",
       "11  LLaMA 3     DocArray      True  Few-shot prompting        0     0\n",
       "12  LLaMA 3       Chroma     False             Default        0     0\n",
       "13  LLaMA 3       Chroma     False  Few-shot prompting        0     0\n",
       "14  LLaMA 3       Chroma      True             Default        0     0\n",
       "15  LLaMA 3       Chroma      True  Few-shot prompting        0     0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
