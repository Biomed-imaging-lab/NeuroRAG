{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment variables\n",
    "\n",
    "You have to define the following environment variables in the `.env` file, terminal environment, or input field within this Jupyter notebook:\n",
    "1. MISTRAL_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_variables = [\n",
    "  'MISTRAL_API_KEY',\n",
    "  'OPENAI_API_KEY',\n",
    "]\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "for key in env_variables:\n",
    "  value = os.getenv(key)\n",
    "\n",
    "  if value is None:\n",
    "    value = getpass(key)\n",
    "\n",
    "  os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK dictionaries\n",
    "\n",
    "These dictionaries are needed for further text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ids = [\n",
    "  'punkt_tab',\n",
    "  'punkt',\n",
    "  'stopwords',\n",
    "  'wordnet',\n",
    "]\n",
    "\n",
    "for dict_id in dict_ids:\n",
    "  nltk.download(dict_id, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Define a function for text preprocessing, which is an important step before calculating any metrics. This preprocessing function will help in cleaning the text data, making it ready for further analysis. The preprocessing involves several steps:\n",
    "1. Lowercasing\n",
    "2. Stopwords removal\n",
    "3. Lemmatization\n",
    "4. Remove accents from characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(corpus: str) -> str:\n",
    "  corpus = corpus.lower()\n",
    "  stopset = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('russian') + list(string.punctuation)\n",
    "  tokens = nltk.word_tokenize(corpus)\n",
    "  tokens = [t for t in tokens if t not in stopset]\n",
    "  tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "  corpus = ' '.join(tokens)\n",
    "  corpus = unidecode(corpus)\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Initialization\n",
    "\n",
    "Here we are initializing the Llama 3 embeddings model. The `OllamaEmbeddings` class is a component of the Ollama library, a set of pre-trained language models. This model is capable of embedding corpora of any length into a 4096-dimensional vector.\n",
    "\n",
    "The use of `OllamaEmbeddings` requires the installation of a local Ollama server, which can be found at https://ollama.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama3.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average embeddings cosine similarity metric\n",
    "\n",
    "This function calculates the average cosine similarity between expected answers and LLM predicted answers using their respective embeddings. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them:\n",
    "\n",
    "$$\n",
    "K(a, b) = \\frac{\\sum \\limits_{i=1}^n a_i b_i}{\\sqrt{\\sum \\limits_{i=1}^n a_i^2} \\cdot \\sqrt{\\sum \\limits_{i=1}^n b_i^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_cosine_sim_metric(expected_answers: list[str], predicted_answers: list[str]) -> float:\n",
    "  results = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    expected_embedding = np.array(embeddings.embed_query(expected_answer))\n",
    "    predicted_embedding = np.array(embeddings.embed_query(predicted_answer))\n",
    "\n",
    "    sim = cosine_similarity(\n",
    "      expected_embedding.reshape(1, -1),\n",
    "      predicted_embedding.reshape(1, -1),\n",
    "    )[0][0]\n",
    "\n",
    "    results.append(sim)\n",
    "\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU Metric\n",
    "\n",
    "This function calculates the average BLEU (Bilingual Evaluation Understudy) score between expected answers and predicted answers. The BLEU score is a measure that compares a candidate translation of text to one or more reference translations.\n",
    "\n",
    "A smoothing function is defined to calculate the BLEU score. Smoothing is useful when a perfect match is not found. It ensures that the BLEU scores aren't zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothie_f = nltk.translate.bleu_score.SmoothingFunction().method4\n",
    "\n",
    "def bleu_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    predicted_tokens = nltk.word_tokenize(predicted_answer)\n",
    "    expected_tokens = [nltk.word_tokenize(expected_answer)]\n",
    "\n",
    "    score = nltk.translate.bleu_score.sentence_bleu(\n",
    "      expected_tokens,\n",
    "      predicted_tokens,\n",
    "      smoothing_function=smoothie_f,\n",
    "    )\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROGUE-1 (Recall-Oriented Understudy for Gisting Evaluation 1-gram Scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_1_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rogue_1_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_1_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rouge1'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROGUE-L (Recall-Oriented Understudy for Gisting Evaluation Longest Common Subsequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_l_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def rogue_l_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_l_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rougeL'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama3_1_llm(temperature=0):\n",
    "  return Ollama(model='llama3.1', temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mistral_large_llm(temperature=0):\n",
    "  return ChatMistralAI(\n",
    "    model='mistral-large-latest',\n",
    "    temperature=temperature,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatgpt_40_mini_llm(temperature=0):\n",
    "  return ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=temperature,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenBioLLM 70B Q2_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openbiollm_70_Q2k_llm(temperature=0):\n",
    "  return Ollama(model='taozhiyuai/openbiollm-llama-3:70b_q2_k', temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biomistral 7B Q4_k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biomistral_q4_k_m_llm(temperature=0):\n",
    "  return Ollama(model='cniongolo/biomistral', temperature=temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the afferent cranial nerve nuclei?</td>\n",
       "      <td>Trigeminal sensory nucleus- fibres carry gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the order of the cranial nerves ?</td>\n",
       "      <td>1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the efferent cranial nerve nuclei?</td>\n",
       "      <td>Edinger-westphal nucleus\\nOculomotor nucleus\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which nuclei share the embryo logical origin -...</td>\n",
       "      <td>Oculomotor nucleus Trochlear nucleus Abducens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which nuclei share the embryo logical origin- ...</td>\n",
       "      <td>Trigeminal motor nucleus Facial motor nucleus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>What is the purpose of gephyrin in the glycine...</td>\n",
       "      <td>Involved in anchoring the receptor to a specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>What is the glycine receptor involved in ?</td>\n",
       "      <td>Reflex response\\nCauses reciprocal inhibition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>What happens in hyperperplexia ?</td>\n",
       "      <td>It’s an exaggerated reflex Often caused by a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>What is hyperperplexia treated with ?</td>\n",
       "      <td>Benzodiazepine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>What increases glycine release ?</td>\n",
       "      <td>Tetanus toxin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1052 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0           What are the afferent cranial nerve nuclei?   \n",
       "1             What is the order of the cranial nerves ?   \n",
       "2           What are the efferent cranial nerve nuclei?   \n",
       "3     Which nuclei share the embryo logical origin -...   \n",
       "4     Which nuclei share the embryo logical origin- ...   \n",
       "...                                                 ...   \n",
       "1047  What is the purpose of gephyrin in the glycine...   \n",
       "1048         What is the glycine receptor involved in ?   \n",
       "1049                   What happens in hyperperplexia ?   \n",
       "1050              What is hyperperplexia treated with ?   \n",
       "1051                   What increases glycine release ?   \n",
       "\n",
       "                                                 answer  \n",
       "0     Trigeminal sensory nucleus- fibres carry gener...  \n",
       "1     1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...  \n",
       "2     Edinger-westphal nucleus\\nOculomotor nucleus\\n...  \n",
       "3     Oculomotor nucleus Trochlear nucleus Abducens ...  \n",
       "4     Trigeminal motor nucleus Facial motor nucleus ...  \n",
       "...                                                 ...  \n",
       "1047  Involved in anchoring the receptor to a specif...  \n",
       "1048  Reflex response\\nCauses reciprocal inhibition ...  \n",
       "1049  It’s an exaggerated reflex Often caused by a m...  \n",
       "1050                                     Benzodiazepine  \n",
       "1051                                      Tetanus toxin  \n",
       "\n",
       "[1052 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.read_csv('../datasets/brainscape.csv')\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup experiment grid search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = (\n",
    "  ('GPT-4o mini', get_chatgpt_40_mini_llm()),\n",
    "  ('Mistral Large', get_mistral_large_llm()),\n",
    "  ('LLaMA 3.1', get_llama3_1_llm()),\n",
    "  ('OpenBioLLM 70B Q2_k', get_openbiollm_70_Q2k_llm()),\n",
    "  ('Biomistral 7B Q4_k_m', get_biomistral_q4_k_m_llm()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Questions: 100%|██████████| 1052/1052 [00:00<00:00, 2504204.20it/s]\n",
      "Questions: 100%|██████████| 1052/1052 [00:00<00:00, 2702025.60it/s]\n",
      "Questions: 100%|██████████| 1052/1052 [00:00<00:00, 2718673.94it/s]\n",
      "Questions: 100%|██████████| 1052/1052 [00:00<00:00, 2868925.75it/s]\n",
      "Questions: 100%|██████████| 1052/1052 [00:00<00:00, 2914404.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm</th>\n",
       "      <th>cos_sim</th>\n",
       "      <th>bleu</th>\n",
       "      <th>rogue_1</th>\n",
       "      <th>rogue_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenBioLLM 70B Q2_k</td>\n",
       "      <td>0.551966</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.181873</td>\n",
       "      <td>0.158006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biomistral 7B Q4_k_m</td>\n",
       "      <td>0.549170</td>\n",
       "      <td>0.011677</td>\n",
       "      <td>0.161228</td>\n",
       "      <td>0.138833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4o mini</td>\n",
       "      <td>0.468678</td>\n",
       "      <td>0.007517</td>\n",
       "      <td>0.216079</td>\n",
       "      <td>0.182296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mistral Large</td>\n",
       "      <td>0.466720</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.213689</td>\n",
       "      <td>0.180652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LLaMA 3.1</td>\n",
       "      <td>0.465571</td>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.185170</td>\n",
       "      <td>0.159363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    llm   cos_sim      bleu   rogue_1   rogue_l\n",
       "3   OpenBioLLM 70B Q2_k  0.551966  0.014683  0.181873  0.158006\n",
       "4  Biomistral 7B Q4_k_m  0.549170  0.011677  0.161228  0.138833\n",
       "0           GPT-4o mini  0.468678  0.007517  0.216079  0.182296\n",
       "1         Mistral Large  0.466720  0.006592  0.213689  0.180652\n",
       "2             LLaMA 3.1  0.465571  0.006327  0.185170  0.159363"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "questions = qa_df['question'].tolist()\n",
    "expected_answers = qa_df['answer'].tolist()\n",
    "\n",
    "for llm_name, llm in llms:\n",
    "  chain = llm | StrOutputParser()\n",
    "\n",
    "  predicted_answers = []\n",
    "\n",
    "  for question in tqdm(questions, desc='Questions'):\n",
    "    response = chain.invoke(question)\n",
    "\n",
    "    predicted_answers.append(response)\n",
    "\n",
    "  # Evaluate metrics\n",
    "  cos_sim = embeddings_cosine_sim_metric(expected_answers, predicted_answers)\n",
    "  bleu_score = bleu_metric(expected_answers, predicted_answers)\n",
    "  rogue_1_score = rogue_1_metric(expected_answers, predicted_answers)\n",
    "  rogue_l_score = rogue_l_metric(expected_answers, predicted_answers)\n",
    "\n",
    "  # Save results\n",
    "  row = pd.DataFrame({\n",
    "    'llm': llm_name,\n",
    "    'cos_sim': cos_sim,\n",
    "    'bleu': bleu_score,\n",
    "    'rogue_1': rogue_1_score,\n",
    "    'rogue_l': rogue_l_score,\n",
    "  }, index=[0])\n",
    "  df = pd.concat([df, row], ignore_index=True)\n",
    "\n",
    "df.sort_values(by='cos_sim', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
