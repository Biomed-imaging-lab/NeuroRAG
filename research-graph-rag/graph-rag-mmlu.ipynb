{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: unidecode in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (1.3.8)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (1.5.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (4.66.2)\n",
      "Requirement already satisfied: llm-blender in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: rouge-score in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: xmltodict in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.13.0)\n",
      "Requirement already satisfied: arxiv in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: biopython in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (1.84)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from llm-blender) (4.41.2)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from llm-blender) (2.3.0)\n",
      "Requirement already satisfied: accelerate in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from llm-blender) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from llm-blender) (0.4.3)\n",
      "Requirement already satisfied: dataclasses-json in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from llm-blender) (0.6.6)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from llm-blender) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from llm-blender) (5.29.3)\n",
      "Requirement already satisfied: absl-py in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2024.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from accelerate->llm-blender) (23.2)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from accelerate->llm-blender) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from accelerate->llm-blender) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from accelerate->llm-blender) (0.23.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch->llm-blender) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch->llm-blender) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch->llm-blender) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch->llm-blender) (2.6.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch->llm-blender) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from torch->llm-blender) (2024.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from dataclasses-json->llm-blender) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from dataclasses-json->llm-blender) (0.9.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from transformers->llm-blender) (0.19.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->llm-blender) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from jinja2->torch->llm-blender) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sympy->torch->llm-blender) (1.3.0)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: langchain in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain-core in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-community in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.3.9)\n",
      "Requirement already satisfied: langchain_experimental in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain-openai in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.2.11)\n",
      "Requirement already satisfied: langchain-chroma in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.1.4)\n",
      "Requirement already satisfied: langchain_mistralai in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.2.3)\n",
      "Requirement already satisfied: langgraph in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.2.56)\n",
      "Requirement already satisfied: langchainhub in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (0.1.21)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (2.0.31)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (0.1.138)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-core) (4.11.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-community) (0.6.6)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-community) (2.6.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-openai) (1.54.5)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-chroma) (0.5.3)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain-chroma) (0.111.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain_mistralai) (0.27.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchain_mistralai) (0.19.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langgraph) (2.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langgraph) (0.1.43)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langchainhub) (2.32.0.20240521)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.7.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.30.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.5.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.18.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.4)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.69.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (30.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.3)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (3.1.4)\n",
      "Requirement already satisfied: python-multipart>=0.0.7 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.0.9)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (2.2.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.4->langgraph) (1.1.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.23.0)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.1.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from email_validator>=2.0.0->fastapi<1,>=0.95.2->langchain-chroma) (2.6.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from jinja2>=2.11.2->fastapi<1,>=0.95.2->langchain-chroma) (2.1.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.32.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: coloredlogs in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.29.3)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.12)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.0.0,>=6.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.11.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.63.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.26.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.26.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.26.0)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma)\n",
      "  Downloading protobuf-4.25.6-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.47b0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.47b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (69.5.1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.7.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.15.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.0)\n",
      "Downloading protobuf-4.25.6-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.3\n",
      "    Uninstalling protobuf-5.29.3:\n",
      "      Successfully uninstalled protobuf-5.29.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.69.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-4.25.6\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk numpy pandas unidecode scikit-learn tqdm llm-blender rouge-score xmltodict arxiv biopython\n",
    "! pip install langchain langchain-core langchain-community langchain_experimental langchain-openai langchain-chroma langchain_mistralai langgraph langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import llm_blender\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from Bio import Entrez, SeqIO\n",
    "import torch\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.llms import Ollama\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import RetryOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import PubMedRetriever, ArxivRetriever\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment variables\n",
    "\n",
    "You have to define the following environment variables in the `.env` file, terminal environment, or input field within this Jupyter notebook:\n",
    "1. MISTRAL_API_KEY\n",
    "2. OPENAI_API_KEY\n",
    "3. OPENAI_PROXY\n",
    "4. TAVILY_API_KEY\n",
    "5. ENTREZ_EMAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_variables = [\n",
    "  'MISTRAL_API_KEY',\n",
    "  'OPENAI_API_KEY',\n",
    "  'OPENAI_PROXY',\n",
    "  'TAVILY_API_KEY',\n",
    "  'ENTREZ_EMAIL',\n",
    "]\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "for key in env_variables:\n",
    "  value = os.getenv(key)\n",
    "\n",
    "  if value is None:\n",
    "    value = getpass(key)\n",
    "\n",
    "  os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK dictionaries\n",
    "\n",
    "These dictionaries are needed for further text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ids = [\n",
    "  'punkt_tab',\n",
    "  'punkt',\n",
    "  'stopwords',\n",
    "  'wordnet',\n",
    "]\n",
    "\n",
    "for dict_id in dict_ids:\n",
    "  nltk.download(dict_id, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Define a function for text preprocessing, which is an important step before calculating any metrics. This preprocessing function will help in cleaning the text data, making it ready for further analysis. The preprocessing involves several steps:\n",
    "1. Lowercasing\n",
    "2. Stopwords removal\n",
    "3. Lemmatization\n",
    "4. Remove accents from characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(corpus: str) -> str:\n",
    "  corpus = corpus.lower()\n",
    "  stopset = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('russian') + list(string.punctuation)\n",
    "  tokens = nltk.word_tokenize(corpus)\n",
    "  tokens = [t for t in tokens if t not in stopset]\n",
    "  tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "  corpus = ' '.join(tokens)\n",
    "  corpus = unidecode(corpus)\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Initialization\n",
    "\n",
    "Here we are initializing the Llama 3 embeddings model. The `OllamaEmbeddings` class is a component of the Ollama library, a set of pre-trained language models. This model is capable of embedding corpora of any length into a 4096-dimensional vector.\n",
    "\n",
    "The use of `OllamaEmbeddings` requires the installation of a local Ollama server, which can be found at https://ollama.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama3.1')\n",
    "store = LocalFileStore(\"./.embeddings_cache\")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "  embeddings,\n",
    "  store,\n",
    "  namespace=embeddings.model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average embeddings cosine similarity metric\n",
    "\n",
    "This function calculates the average cosine similarity between expected answers and LLM predicted answers using their respective embeddings. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them:\n",
    "\n",
    "$$\n",
    "K(a, b) = \\frac{\\sum \\limits_{i=1}^n a_i b_i}{\\sqrt{\\sum \\limits_{i=1}^n a_i^2} \\cdot \\sqrt{\\sum \\limits_{i=1}^n b_i^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_cosine_sim_metric(expected_answers: list[str], predicted_answers: list[str]) -> float:\n",
    "  results = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    expected_embedding = np.array(cached_embeddings.embed_query(expected_answer))\n",
    "    predicted_embedding = np.array(cached_embeddings.embed_query(predicted_answer))\n",
    "\n",
    "    sim = cosine_similarity(\n",
    "      expected_embedding.reshape(1, -1),\n",
    "      predicted_embedding.reshape(1, -1),\n",
    "    )[0][0]\n",
    "\n",
    "    results.append(sim)\n",
    "\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothie_f = nltk.translate.bleu_score.SmoothingFunction().method4\n",
    "\n",
    "def bleu_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    predicted_tokens = nltk.word_tokenize(predicted_answer)\n",
    "    expected_tokens = [nltk.word_tokenize(expected_answer)]\n",
    "\n",
    "    score = nltk.translate.bleu_score.sentence_bleu(\n",
    "      expected_tokens,\n",
    "      predicted_tokens,\n",
    "      smoothing_function=smoothie_f,\n",
    "    )\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_1_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rogue_1_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_1_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rouge1'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_l_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def rogue_l_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_l_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rougeL'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4438"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dir = Path('./docs')\n",
    "docs_cache_dir = Path('./.docs_cache')\n",
    "raw_docs_pkl_path = docs_cache_dir / 'parsed_docs_cache.pkl'\n",
    "\n",
    "if os.path.exists(raw_docs_pkl_path):\n",
    "  with open(raw_docs_pkl_path, 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "else:\n",
    "  docs = []\n",
    "\n",
    "  for file in docs_dir.iterdir():\n",
    "    file_docs = PDFMinerLoader(file, concatenate_pages=False).load()\n",
    "    for doc in file_docs:\n",
    "      doc.page_content = unidecode(doc.page_content)\n",
    "      page = doc.metadata['page']\n",
    "      doc.metadata['source'] = f'{file.stem} ({page})'\n",
    "    docs.extend(file_docs)\n",
    "\n",
    "  with open(raw_docs_pkl_path, 'wb') as f:\n",
    "    pickle.dump(docs, f)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35663"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_docs_pkl_path = docs_cache_dir / 'splitted_docs_cache.pkl'\n",
    "\n",
    "if os.path.exists(splitted_docs_pkl_path):\n",
    "  with open(splitted_docs_pkl_path, 'rb') as f:\n",
    "    splitted_docs = pickle.load(f)\n",
    "else:\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=750,\n",
    "    chunk_overlap=250,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    separators=[\n",
    "      '.',\n",
    "      '\\uff0e', # Fullwidth full stop\n",
    "      '\\u3002', # Ideographic full stop\n",
    "      '\\n\\n',\n",
    "    ],\n",
    "  )\n",
    "  splitted_docs = text_splitter.create_documents([doc.page_content for doc in docs])\n",
    "\n",
    "  with open(splitted_docs_pkl_path, 'wb') as f:\n",
    "    pickle.dump(splitted_docs, f)\n",
    "\n",
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "  collection_name='neurorag',\n",
    "  embedding_function=cached_embeddings,\n",
    "  persist_directory='./chroma_db'\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define JSON extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(response):\n",
    "  json_pattern = r'\\{.*?\\}'\n",
    "  match = re.search(json_pattern, response, re.DOTALL)\n",
    "\n",
    "  if match:\n",
    "    return match.group().strip().replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='llama3.1', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sources=[]\n",
      "sources=['vectorstore', 'ncbi_gene']\n"
     ]
    }
   ],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "  sources: List[str] = Field(\n",
    "    description='Given a user question select the retrieval methods you consider the most appropriate for addressing this question. You may also return an empty array if no methods are required.',\n",
    "  )\n",
    "\n",
    "route_parser = PydanticOutputParser(pydantic_object=RouteQuery)\n",
    "route_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=route_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "route_template = \"\"\"\n",
    "You are an expert at selecting retrieval methods.\n",
    "Given a user question select the retrieval methods you consider the most appropriate for addressing user question.\n",
    "You may also return an empty array if no methods are required.\n",
    "\n",
    "Possible retrieval methods:\n",
    "1. The \"vectorstore\" retriever contains documents related to neurobiology and medicine. Use the vectorstore for questions on these topics.\n",
    "2. The \"pubmed\" retriever contains biomedical literature and research articles. It is particularly useful for answering detailed questions about medical research, clinical studies, and scientific discoveries.\n",
    "3. The \"arxiv\" retriever contains preprints of research papers across various scientific fields, including physics, mathematics, computer science, and biology. Use the arxiv for questions on recent scientific research and theoretical studies in these areas.\n",
    "4. The \"ncbi_protein\" retriever contains protein sequence and functional information. Use the NCBI protein DB for questions related to protein sequences, structures, and functions.\n",
    "5. The \"ncbi_gene\" retriever contains gene sequence and functional information. Use the NCBI gene DB for questions related to gene sequences, structures, and functions.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\"\"\"\n",
    "route_prompt = PromptTemplate(\n",
    "  template=route_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': route_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "question_router = RunnableParallel(\n",
    "  completion=route_prompt | llm | extract_json, prompt_value=route_prompt\n",
    ") | RunnableLambda(lambda x: route_retry_parser.parse_with_prompt(**x))\n",
    "print(question_router.invoke({'question': 'Who will the Bears draft first in the NFL draft?'}))\n",
    "print(question_router.invoke({'question': 'What are the functions of the oculomotor nerve?'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade documents chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeDocuments(binary_score='yes')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "  binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "docs_grader_parser = PydanticOutputParser(pydantic_object=GradeDocuments)\n",
    "docs_grader_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=docs_grader_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "docs_grader_template = \"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Retrieved document:\n",
    "{document}\n",
    "\"\"\"\n",
    "docs_grader_prompt = PromptTemplate(\n",
    "  template=docs_grader_template,\n",
    "  input_variables=['document', 'question'],\n",
    "  partial_variables={'format_instructions': docs_grader_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "docs_grader_grader = RunnableParallel(\n",
    "  completion=docs_grader_prompt | llm | extract_json, prompt_value=docs_grader_prompt\n",
    ") | RunnableLambda(lambda x: docs_grader_retry_parser.parse_with_prompt(**x))\n",
    "docs_grader_grader.invoke({'question': 'What is the color of the sky?', 'document': 'The color of the sky is blue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "  binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "hallucination_parser = PydanticOutputParser(pydantic_object=GradeHallucinations)\n",
    "hallucination_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=hallucination_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "hallucination_template = \"\"\"\n",
    "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Set of facts:\n",
    "{documents}\n",
    "\n",
    "LLM generation:\n",
    "{generation}\n",
    "\"\"\"\n",
    "hallucination_prompt = PromptTemplate(\n",
    "  template=hallucination_template,\n",
    "  input_variables=['question', 'generation'],\n",
    "  partial_variables={'format_instructions': hallucination_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "hallucination_grader = RunnableParallel(\n",
    "  completion=hallucination_prompt | llm | extract_json, prompt_value=hallucination_prompt\n",
    ") | RunnableLambda(lambda x: hallucination_retry_parser.parse_with_prompt(**x))\n",
    "print(hallucination_grader.invoke({'documents': ['Sky is blue'], 'generation': 'The color of the sky is blue'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer grade chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "  binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_parser = PydanticOutputParser(pydantic_object=GradeAnswer)\n",
    "grade_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=grade_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "grade_template = \"\"\"\n",
    "You are a grader assessing whether an answer addresses / resolves a question. \\n\n",
    "Give a binary score 'yes' or 'no'. 'yes' means that the answer resolves the question.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "LLM generation:\n",
    "{generation}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "grade_prompt = PromptTemplate(\n",
    "  template=grade_template,\n",
    "  input_variables=['question', 'generation'],\n",
    "  partial_variables={'format_instructions': grade_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "answer_grader = RunnableParallel(\n",
    "  completion=grade_prompt | llm | extract_json, prompt_value=grade_prompt\n",
    ") | RunnableLambda(lambda x: grade_retry_parser.parse_with_prompt(**x))\n",
    "print(answer_grader.invoke({\"question\": \"What is the order of the cranial nerves?\", 'generation': 'I do not know.'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a scientific paper-style passage answering the question:\\n\\n**Title:** The Cranial Nerve Order: A Review and Classification\\n\\n**Abstract:**\\n\\nThe cranial nerves are a complex group of 12 pairs of nerves that arise directly from the brain, playing crucial roles in various physiological functions. Despite their importance, the order of these nerves has been a subject of interest for centuries. This review aims to provide an overview of the classification and nomenclature of the cranial nerves, highlighting their distinct characteristics and functions.\\n\\n**Introduction:**\\n\\nThe cranial nerves are a unique group of nerves that emerge from the brain, serving as the primary means of communication between the central nervous system and various peripheral structures. The order of these nerves has been a topic of debate among neuroscientists, with different classification systems proposed over the years. In this review, we will present an updated classification scheme for the cranial nerves, highlighting their distinct characteristics and functions.\\n\\n**The Order of the Cranial Nerves:**\\n\\nThe 12 pairs of cranial nerves are traditionally listed in the following order:\\n\\n1. **Olfactory nerve (I)**: The first cranial nerve to develop, responsible for transmitting sensory information from the olfactory epithelium.\\n2. **Optic nerve (II)**: A paired nerve that transmits visual information from the retina to the brain.\\n3. **Occulomotor nerve (III)**: A large motor nerve that controls eye movement and pupillary constriction.\\n4. **Trochlear nerve (IV)**: The thinnest cranial nerve, responsible for controlling superior oblique muscle function.\\n5. **Trigeminal nerve (V)**: A complex nerve with three branches (ophthalmic, maxillary, and mandibular) that transmit sensory information from the face and motor signals to the muscles of mastication.\\n6. **Abducens nerve (VI)**: A small motor nerve that controls lateral rectus muscle function.\\n7. **Facial nerve (VII)**: A mixed nerve responsible for transmitting taste, smell, and auditory information, as well as controlling facial expressions and salivation.\\n8. **Vestibulocochlear nerve (VIII)**: A paired nerve that transmits auditory and vestibular information from the inner ear to the brain.\\n9. **Glossopharyngeal nerve (IX)**: A mixed nerve responsible for transmitting sensory information from the pharynx, tongue, and ear, as well as controlling swallowing and salivation.\\n10. **Vagus nerve (X)**: A complex nerve with multiple branches that transmit sensory information from various organs and control various physiological functions, including respiration and digestion.\\n11. **Spinal accessory nerve (XI)**: A motor nerve responsible for controlling sternocleidomastoid and trapezius muscle function.\\n12. **Hypoglossal nerve (XII)**: The smallest cranial nerve, responsible for controlling tongue movement.\\n\\n**Conclusion:**\\n\\nIn conclusion, the order of the cranial nerves is a well-established classification scheme that has been refined over centuries. Understanding the distinct characteristics and functions of each nerve pair is essential for appreciating their roles in various physiological processes. This review aims to provide a comprehensive overview of the cranial nerve order, highlighting their importance in human anatomy and physiology.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_template = \"\"\"\n",
    "Please write a scientific paper passage to answer the question\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Passage:\n",
    "\"\"\"\n",
    "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "hyde_chain.invoke({\"question\": 'What is the order of the cranial nerves ?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of a neurological or medical syndrome?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StepBackAnswer(BaseModel):\n",
    "  step_back: str = Field(description=\"Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\")\n",
    "\n",
    "\n",
    "step_back_parser = PydanticOutputParser(pydantic_object=StepBackAnswer)\n",
    "step_back_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=step_back_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "step_back_template = \"\"\"\n",
    "You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Step-back query:\n",
    "\"\"\"\n",
    "step_back_prompt = PromptTemplate(\n",
    "  template=step_back_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': step_back_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "step_back_chain = RunnableParallel(\n",
    "  completion=step_back_prompt | llm | extract_json, prompt_value=step_back_prompt\n",
    ") | RunnableLambda(lambda x: step_back_retry_parser.parse_with_prompt(**x))\n",
    "print(step_back_chain.invoke({\"question\": \"What is Benedict’s syndrome?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a rewritten query that\\'s more specific, detailed, and likely to retrieve relevant information:\\n\\n\"What is the correct anatomical order of the 12 pairs of cranial nerves, including their names (I-XII), functions, and any notable characteristics or associations with specific brain regions?\"\\n\\nThis revised query adds more specificity by:\\n\\n* Specifying the number of cranial nerves (12)\\n* Including their names (I-XII) to ensure accurate retrieval\\n* Mentioning their functions to provide context for the information being sought\\n* Asking about notable characteristics or associations, which may help retrieve additional relevant details\\n\\nBy making these changes, the rewritten query is more likely to retrieve precise and detailed information from a RAG system, rather than just a general answer.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RewriteQueryAnswer(BaseModel):\n",
    "  rewritten_query: str = Field(description=\"Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\")\n",
    "\n",
    "\n",
    "rewrite_query_parser = PydanticOutputParser(pydantic_object=RewriteQueryAnswer)\n",
    "rewrite_query_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=rewrite_query_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "rewrite_query_template = \"\"\"\n",
    "You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Rewritten query:\n",
    "\"\"\"\n",
    "rewrite_query_prompt = PromptTemplate(\n",
    "  template=rewrite_query_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': rewrite_query_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "rewrite_query_chain = RunnableParallel(\n",
    "  completion=rewrite_query_prompt | llm | extract_json, prompt_value=rewrite_query_prompt\n",
    ") | RunnableLambda(lambda x: rewrite_query_retry_parser.parse_with_prompt(**x))\n",
    "print(rewrite_query_chain.invoke({'question': 'What is the order of the cranial nerves?'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subqueries=[\"What are the symptoms of Benedict's syndrome?\", \"What are the causes of Benedict's syndrome?\", \"How is Benedict's syndrome diagnosed?\", \"What are the treatment options for Benedict's syndrome?\"]\n"
     ]
    }
   ],
   "source": [
    "class DecompositionAnswer(BaseModel):\n",
    "  subqueries: List[str] = Field(description=\"Given the original query, decompose it into 2-4 simpler sub-queries as json array of strings\")\n",
    "\n",
    "decomposition_parser = PydanticOutputParser(pydantic_object=DecompositionAnswer)\n",
    "decomposition_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=decomposition_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "decomposition_template = \"\"\"\n",
    "You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "decomposition_prompt = PromptTemplate(\n",
    "  template=decomposition_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': decomposition_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "decomposition_chain = RunnableParallel(\n",
    "  completion=decomposition_prompt | llm | extract_json, prompt_value=decomposition_prompt\n",
    ") | RunnableLambda(lambda x: decomposition_retry_parser.parse_with_prompt(**x))\n",
    "print(decomposition_chain.invoke({\"question\": \"What is Benedict’s syndrome?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /home/super-pc2/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "  'cuda'\n",
    "  if torch.cuda.is_available()\n",
    "  else 'mps'\n",
    "  if torch.backends.mps.is_available()\n",
    "  else 'cpu'\n",
    ")\n",
    "blender = llm_blender.Blender()\n",
    "blender.loadranker('llm-blender/PairRM', device=device)\n",
    "blender.loadfuser('llm-blender/gen_fuser_3b', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n",
      "Fusing candidates: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The NMDA receptor is composed of two subunits, GluN1 and GluN2. The GluN1 subunit is essential for the receptor's function and binds glycine, while the GluN2 subunit is responsible for calcium ion influx. The GluN1 and GluN2 subunits interact with each other to form a heterodimer, which is responsible for the receptor's pharmacological properties.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class RAGSchema(BaseModel):\n",
    "  correct_answer: str = Field(\n",
    "    description='Given a question and answer options, provide the corresponding letter for the correct answer..',\n",
    "  )\n",
    "\n",
    "rag_parser = PydanticOutputParser(pydantic_object=RAGSchema)\n",
    "\n",
    "rag_template = \"\"\"Answer the following multiple choice question by giving the most appropriate response in json format. Answer should be one among [A, B, C, D].\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Question: {question}\\n\n",
    "A) {a}\\n\n",
    "B) {b}\\n\n",
    "C) {c}\\n\n",
    "D) {d}\\n\n",
    "\n",
    "Context:\n",
    "\n",
    "{choices}\n",
    "\"\"\"\n",
    "rag_prompt = PromptTemplate(\n",
    "  template=rag_template,\n",
    "  input_variables=['question', 'a', 'b', 'c', 'd', 'context'],\n",
    "  partial_variables={'format_instructions': rag_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "gpt_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "openbio_llm = Ollama(model='taozhiyuai/openbiollm-llama-3:70b_q2_k', temperature=0)\n",
    "biomistral_llm = Ollama(model='cniongolo/biomistral', temperature=0)\n",
    "\n",
    "gpt_chain = rag_prompt | gpt_llm | StrOutputParser()\n",
    "openbio_chain = rag_prompt | openbio_llm | StrOutputParser()\n",
    "biomistral_chain = rag_prompt | biomistral_llm | StrOutputParser()\n",
    "\n",
    "def fuse_generations(dict):\n",
    "  question = dict['question']\n",
    "\n",
    "  gpt_res = dict['gpt_res']\n",
    "  openbio_res = dict['openbio_res']\n",
    "  biomistral_res = dict['biomistral_res']\n",
    "  answers = [gpt_res, openbio_res, biomistral_res]\n",
    "\n",
    "  fuse_generations, ranks = blender.rank_and_fuse(\n",
    "    [question],\n",
    "    [answers],\n",
    "    instructions=['keep the similar length of the output as the candidates.'],\n",
    "    return_scores=False,\n",
    "    batch_size=1,\n",
    "    top_k=5,\n",
    "  )\n",
    "  return fuse_generations[0]\n",
    "\n",
    "rag_chain = (\n",
    "  {\n",
    "    'gpt_res': gpt_chain,\n",
    "    'openbio_res': openbio_chain,\n",
    "    'biomistral_res': biomistral_chain,\n",
    "    'question': itemgetter('question')\n",
    "  }\n",
    "  | RunnableLambda(fuse_generations)\n",
    ")\n",
    "\n",
    "final_res = rag_chain.invoke({\"context\": '', \"question\": 'What is subunit composition of NMDA receptors and role of each subunit?'})\n",
    "print(final_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = TavilySearchResults(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_med_retriever = PubMedRetriever()\n",
    "pub_med_retriever.invoke('What is the order of the cranial nerves?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/1912.10601v2', 'Published': datetime.date(2021, 3, 13), 'Title': 'Optimized Cranial Bandeau Remodeling', 'Authors': 'James Drake, Marina Drygala, Ricardo Fukasawa, Jochen Koenemann, Andre Linhares, Thomas Looi, John Phillips, David Qian, Nikoo Saber, Justin Toth, Chris Woodbeck, Jessie Yeung'}, page_content=\"Craniosynostosis, a condition affecting 1 in 2000 infants, is caused by\\npremature fusing of cranial vault sutures, and manifests itself in abnormal\\nskull growth patterns. Left untreated, the condition may lead to severe\\ndevelopmental impairment. Standard practice is to apply corrective cranial\\nbandeau remodeling surgery in the first year of the infant's life. The most\\nfrequent type of surgery involves the removal of the so-called fronto-orbital\\nbar from the patient's forehead and the cutting of well-placed incisions to\\nreshape the skull in order to obtain the desired result. In this paper, we\\npropose a precise optimization model for the above cranial bandeau remodeling\\nproblem and its variants. We have developed efficient algorithms that solve\\nbest incision placement, and show hardness for more general cases in the class.\\nTo the best of our knowledge this paper is the first to introduce optimization\\nmodels for craniofacial surgery applications.\"),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/1706.07649v1', 'Published': datetime.date(2017, 6, 23), 'Title': 'Computer-aided implant design for the restoration of cranial defects', 'Authors': 'Xiaojun Chen, Lu Xu, Xing Li, Jan Egger'}, page_content='Patient-specific cranial implants are important and necessary in the surgery\\nof cranial defect restoration. However, traditional methods of manual design of\\ncranial implants are complicated and time-consuming. Our purpose is to develop\\na novel software named EasyCrania to design the cranial implants conveniently\\nand efficiently. The process can be divided into five steps, which are\\nmirroring model, clipping surface, surface fitting, the generation of the\\ninitial implant and the generation of the final implant. The main concept of\\nour method is to use the geometry information of the mirrored model as the base\\nto generate the final implant. The comparative studies demonstrated that the\\nEasyCrania can improve the efficiency of cranial implant design significantly.\\nAnd, the intra- and inter-rater reliability of the software were stable, which\\nwere 87.07+/-1.6% and 87.73+/-1.4% respectively.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2009.13704v1', 'Published': datetime.date(2020, 9, 29), 'Title': 'Cranial Implant Design via Virtual Craniectomy with Shape Priors', 'Authors': 'Franco Matzkin, Virginia Newcombe, Ben Glocker, Enzo Ferrante'}, page_content='Cranial implant design is a challenging task, whose accuracy is crucial in\\nthe context of cranioplasty procedures. This task is usually performed manually\\nby experts using computer-assisted design software. In this work, we propose\\nand evaluate alternative automatic deep learning models for cranial implant\\nreconstruction from CT images. The models are trained and evaluated using the\\ndatabase released by the AutoImplant challenge, and compared to a baseline\\nimplemented by the organizers. We employ a simulated virtual craniectomy to\\ntrain our models using complete skulls, and compare two different approaches\\ntrained with this procedure. The first one is a direct estimation method based\\non the UNet architecture. The second method incorporates shape priors to\\nincrease the robustness when dealing with out-of-distribution implant shapes.\\nOur direct estimation method outperforms the baselines provided by the\\norganizers, while the model with shape priors shows superior performance when\\ndealing with out-of-distribution cases. Overall, our methods show promising\\nresults in the difficult task of cranial implant design.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_retriever = ArxivRetriever(load_max_docs=3, get_ful_documents=True)\n",
    "arxiv_retriever.invoke('What is the order of the cranial nerves?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI Protein DB retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_params = {\n",
    "  'gene': {\n",
    "    'rettype': 'xml',\n",
    "    'retmode': 'xml',\n",
    "  },\n",
    "  'protein': {\n",
    "    'rettype': 'gb',\n",
    "    'retmode': 'text',\n",
    "  },\n",
    "}\n",
    "\n",
    "class NCBIRetriever(BaseRetriever):\n",
    "  db: str\n",
    "  k: int\n",
    "\n",
    "  def __init__(self, db: str, k: int):\n",
    "    super().__init__(db=db, k=k)\n",
    "\n",
    "    self.db = db\n",
    "    self.k = k\n",
    "\n",
    "    entrez_email = os.getenv('ENTREZ_EMAIL')\n",
    "    if entrez_email == None:\n",
    "      raise ValueError('ENTREZ_EMAIL is not defined')\n",
    "    Entrez.email = entrez_email\n",
    "\n",
    "  def _search(self, term):\n",
    "    handle = Entrez.esearch(db=self.db, term=term, retmax=self.k)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return record['IdList']\n",
    "\n",
    "  def _fetch(self, ids):\n",
    "    rettype = db_params[self.db][\"rettype\"]\n",
    "    retmode = db_params[self.db][\"retmode\"]\n",
    "\n",
    "    handle = Entrez.efetch(db=self.db, id=ids, rettype=rettype, retmode=retmode)\n",
    "    if self.db == 'gene':\n",
    "      records = Entrez.read(handle)\n",
    "    else:\n",
    "      records = [SeqIO.read(handle, rettype)]\n",
    "    handle.close()\n",
    "    return records\n",
    "\n",
    "  def _get_gene_document(self, record):\n",
    "    gene_id = record['Entrezgene_track-info']['Gene-track']['Gene-track_geneid']\n",
    "    gene_symbol = record['Entrezgene_gene']['Gene-ref']['Gene-ref_locus']\n",
    "    gene_description = record.get('Entrezgene_summary', 'N/A')\n",
    "    organism_name = record['Entrezgene_source']['BioSource']['BioSource_org']['Org-ref']['Org-ref_taxname']\n",
    "    page_content = (\n",
    "      f'Gene ID: {gene_id}\\n'\n",
    "      f'Gene Symbol: {gene_symbol}\\n'\n",
    "      f'Organism: {organism_name}\\n'\n",
    "      f'Description: {gene_description}'\n",
    "    )\n",
    "    source = f'https://www.ncbi.nlm.nih.gov/gene/{gene_id}'\n",
    "    document = Document(page_content=page_content, metadata={'source': source})\n",
    "    return document\n",
    "\n",
    "  def _get_protein_document(self, record):\n",
    "    molecule_type = record.annotations.get(\"molecule_type\", \"N/A\")\n",
    "    organism = record.annotations.get(\"organism\", \"N/A\")\n",
    "    comment = record.annotations.get(\"comment\", \"N/A\")\n",
    "    page_content = (\n",
    "      f'Protein ID: {record.id}\\n'\n",
    "      f'Type: {molecule_type}\\n'\n",
    "      f'Name: {record.name}\\n'\n",
    "      f'Organism: {organism}\\n'\n",
    "      f'Description: {record.description}\\n'\n",
    "      f'Comment: {comment}\\n'\n",
    "      f'Sequence: {record.seq}'\n",
    "    )\n",
    "    source = f'https://www.ncbi.nlm.nih.gov/protein/{record.id}'\n",
    "    document = Document(page_content=page_content, metadata={'source': source})\n",
    "    return document\n",
    "\n",
    "  def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "    ids = self._search(query)\n",
    "    records = self._fetch(ids)\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    for record in records:\n",
    "      if self.db == 'gene':\n",
    "        docs.append(self._get_gene_document(record))\n",
    "      elif self.db == 'protein':\n",
    "        docs.append(self._get_protein_document(record))\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/protein/ABW05875.1'}, page_content='Protein ID: ABW05875.1\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncbi_protein_retriever = NCBIRetriever(db='protein', k=3)\n",
    "ncbi_protein_retriever.invoke('ABW05875')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/gene/139103289'}, page_content='Gene ID: 139103289\\nGene Symbol: Peng\\nOrganism: Cardiocondyla obscurior\\nDescription: N/A'),\n",
       " Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/gene/139085911'}, page_content='Gene ID: 139085911\\nGene Symbol: peng\\nOrganism: Chironomus tepperi\\nDescription: N/A'),\n",
       " Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/gene/139048079'}, page_content='Gene ID: 139048079\\nGene Symbol: peng\\nOrganism: Dermacentor albipictus\\nDescription: N/A')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncbi_gene_retriever = NCBIRetriever(db='gene', k=3)\n",
    "ncbi_gene_retriever.invoke('peng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI Protein DB chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/protein/ABW05875.1'}, page_content='Protein ID: ABW05875.1\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD')]\n"
     ]
    }
   ],
   "source": [
    "class NCBIProteinDBAnswer(BaseModel):\n",
    "  query: str = Field(description='Given the original query, please find a protein locus for the NCBI protein database.')\n",
    "\n",
    "ncbi_protein_db_parser = PydanticOutputParser(pydantic_object=NCBIProteinDBAnswer)\n",
    "ncbi_protein_db_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=ncbi_protein_db_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "ncbi_protein_db_template = \"\"\"\n",
    "As an expert in bioinformatics and user query optimization for biological databases, your task is to transform user questions into precise and effective queries suitable for the NCBI protein database.\n",
    "Create a query with only locus of a protein for search within the NCBI protein database.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "ncbi_protein_db_prompt = PromptTemplate(\n",
    "  template=ncbi_protein_db_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': ncbi_protein_db_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "query_extractor = lambda res: res.query\n",
    "\n",
    "ncbi_protein_db_chain = RunnableParallel(\n",
    "  completion=ncbi_protein_db_prompt | llm | extract_json, prompt_value=ncbi_protein_db_prompt\n",
    ") | RunnableLambda(lambda x: ncbi_protein_db_retry_parser.parse_with_prompt(**x)) | query_extractor | ncbi_protein_retriever\n",
    "print(ncbi_protein_db_chain.invoke({\"question\": \"Calculate the frequency of each amino acid in the ABW05875 protein sequence\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI Gene DB chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/gene/139103289'}, page_content='Gene ID: 139103289\\nGene Symbol: Peng\\nOrganism: Cardiocondyla obscurior\\nDescription: N/A'), Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/gene/139085911'}, page_content='Gene ID: 139085911\\nGene Symbol: peng\\nOrganism: Chironomus tepperi\\nDescription: N/A'), Document(metadata={'source': 'https://www.ncbi.nlm.nih.gov/gene/139048079'}, page_content='Gene ID: 139048079\\nGene Symbol: peng\\nOrganism: Dermacentor albipictus\\nDescription: N/A')]\n"
     ]
    }
   ],
   "source": [
    "class NCBIGeneDBAnswer(BaseModel):\n",
    "  query: str = Field(description='Given the original query, please find a gene locus for the NCBI gene database.')\n",
    "\n",
    "ncbi_gene_db_parser = PydanticOutputParser(pydantic_object=NCBIGeneDBAnswer)\n",
    "ncbi_gene_db_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=ncbi_gene_db_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "ncbi_gene_db_template = \"\"\"\n",
    "As an expert in bioinformatics and user query optimization for biological databases, your task is to transform user questions into precise and effective queries suitable for the NCBI gene database.\n",
    "Create a query with only locus of a gene for search within the NCBI gene database.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "ncbi_gene_db_prompt = PromptTemplate(\n",
    "  template=ncbi_gene_db_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': ncbi_gene_db_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "query_extractor = lambda res: res.query\n",
    "\n",
    "ncbi_gene_db_chain = RunnableParallel(\n",
    "  completion=ncbi_gene_db_prompt | llm | extract_json, prompt_value=ncbi_gene_db_prompt\n",
    ") | RunnableLambda(lambda x: ncbi_gene_db_retry_parser.parse_with_prompt(**x)) | query_extractor | ncbi_gene_retriever\n",
    "print(ncbi_gene_db_chain.invoke({\"question\": \"Calculate the frequency of each amino acid in the peng gene sequence\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "  question: str\n",
    "  choices: str\n",
    "\n",
    "  specialized_srcs: List[str]\n",
    "\n",
    "  step_back_query: str\n",
    "  rewritten_query: str\n",
    "  subqueries: List[str]\n",
    "\n",
    "  generated_docs: List[str]\n",
    "\n",
    "  documents: Annotated[list, operator.add]\n",
    "\n",
    "  web_search: str\n",
    "\n",
    "  generation: str\n",
    "  generations_num: int\n",
    "\n",
    "def determine_specialized_srcs(state):\n",
    "  print('---DETERMINE SPECIALIZED SOURCES---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  try:\n",
    "    res = question_router.invoke({'question': question})\n",
    "    srcs = [src.strip().lower() for src in res.sources]\n",
    "  except:\n",
    "    srcs = []\n",
    "\n",
    "  return {'specialized_srcs': srcs}\n",
    "\n",
    "def route_question(state):\n",
    "  print('---ROUTE QUESTION---')\n",
    "\n",
    "  sources = state['specialized_srcs']\n",
    "\n",
    "  if len(sources) == 0:\n",
    "    print('---ROUTE QUESTION TO WEB SEARCH---')\n",
    "    return 'websearch'\n",
    "  else:\n",
    "    print(f'---ROUTE QUESTION TO SPECIALIZED SOURCES: {\", \".join([source.upper() for source in sources])}---')\n",
    "    return 'specialized_srcs'\n",
    "\n",
    "def generate_step_back_query(state):\n",
    "  print('---GENERATE STEP-BACK QUERY---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  try:\n",
    "    response = step_back_chain.invoke({'question': question})\n",
    "    step_back_query = response.step_back\n",
    "  except:\n",
    "    step_back_query = question\n",
    "\n",
    "  return {'step_back_query': step_back_query}\n",
    "\n",
    "def generate_rewritten_query(state):\n",
    "  print('---GENERATE REWRITTEN QUERY---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  try:\n",
    "    response = rewrite_query_chain.invoke({'question': question})\n",
    "    rewritten_query = response.rewritten_query\n",
    "  except:\n",
    "    rewritten_query = question\n",
    "\n",
    "  return {'rewritten_query': rewritten_query}\n",
    "\n",
    "def generate_subqueries(state):\n",
    "  print('---GENERATE SUBQUERIES---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  try:\n",
    "    decomposition_answer = decomposition_chain.invoke({'question': question})\n",
    "    subqueries = decomposition_answer.subqueries\n",
    "    # Limit to a maximum of four subqueries\n",
    "    subqueries = subqueries[:4]\n",
    "  except:\n",
    "    subqueries = []\n",
    "\n",
    "  print(f'---FINAL SUBQUERIES NUMBER: {len(subqueries)}---')\n",
    "\n",
    "  return {'subqueries': subqueries}\n",
    "\n",
    "def generate_hyde_docs(state):\n",
    "  print('---GENERATE HYDE DOCUMENTS---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  generated_docs = []\n",
    "\n",
    "  for query in queries:\n",
    "    generated_doc = hyde_chain.invoke({'question': query})\n",
    "    generated_docs.append(generated_doc)\n",
    "\n",
    "  return {'question': question, 'generated_docs': generated_docs}\n",
    "\n",
    "def vector_store_retriever_node(state):\n",
    "  generated_docs = state['generated_docs']\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'vectorstore' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM VECTOR STORE---')\n",
    "\n",
    "  documents = []\n",
    "\n",
    "  for generated_doc in generated_docs:\n",
    "    documents.extend(retriever.invoke(generated_doc))\n",
    "\n",
    "  return {'documents': documents}\n",
    "\n",
    "def pub_med_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'pubmed' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM PUBMED---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(pub_med_retriever.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return {'documents': documents}\n",
    "\n",
    "def arxiv_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'arxiv' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM ARXIV---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(arxiv_retriever.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return {'documents': documents}\n",
    "\n",
    "def ncbi_protein_db_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'ncbi_protein' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM NCBI PROTEIN DB---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(ncbi_protein_db_chain.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return {'documents': documents}\n",
    "\n",
    "def ncbi_gene_db_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'ncbi_gene' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM NCBI GENE DB---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(ncbi_gene_db_chain.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  return {'documents': documents}\n",
    "\n",
    "def grade_documents(state):\n",
    "  print('---CHECK DOCUMENT RELEVANCE TO QUESTION---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "\n",
    "  print(f'---INITIAL DOCUMENTS NUMBER: {len(documents)}---')\n",
    "\n",
    "  filtered_documents = []\n",
    "  seen_contents = set()\n",
    "  web_search = 'No'\n",
    "\n",
    "  for index, document in enumerate(documents):\n",
    "    print(f'---GRADE DOCUMENT ({index + 1}/{len(documents)})---')\n",
    "\n",
    "    if document.page_content in seen_contents:\n",
    "      print('---GRADE: DOCUMENT IS REPEATED---')\n",
    "      continue\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "    try:\n",
    "      score = docs_grader_grader.invoke({\n",
    "        'question': question,\n",
    "        'document': document.page_content,\n",
    "      })\n",
    "      grade = score.binary_score\n",
    "    except:\n",
    "      grade = 'No'\n",
    "\n",
    "    if grade.lower() == 'yes':\n",
    "      print('---GRADE: DOCUMENT RELEVANT---')\n",
    "      filtered_documents.append(document)\n",
    "    else:\n",
    "      print('---GRADE: DOCUMENT NOT RELEVANT---')\n",
    "      web_search = 'Yes'\n",
    "      continue\n",
    "\n",
    "  print(f'---FINAL DOCUMENTS NUMBER: {len(filtered_documents)}---')\n",
    "\n",
    "  state['documents'].clear()\n",
    "  return {\n",
    "    'documents': filtered_documents,\n",
    "    'web_search': web_search,\n",
    "  }\n",
    "\n",
    "def decide_to_generate(state):\n",
    "  print('---ASSESS GRADED DOCUMENTS---')\n",
    "\n",
    "  web_search = state['web_search']\n",
    "\n",
    "  if web_search == 'Yes':\n",
    "    print('---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---')\n",
    "    return 'websearch'\n",
    "  else:\n",
    "    print('---DECISION: GENERATE---')\n",
    "    return 'generate'\n",
    "\n",
    "def web_search(state):\n",
    "  print('---WEB SEARCH---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  web_results = web_search_tool.invoke({'query': question})\n",
    "  docs = [Document(page_content=result['content'], metadata={'source': result['url']}) for result in web_results]\n",
    "\n",
    "  return {'documents': docs}\n",
    "\n",
    "def generate(state):\n",
    "  print('---GENERATE---')\n",
    "\n",
    "  question = state['question']\n",
    "  choices = state['choices']\n",
    "  documents = state['documents']\n",
    "  generations_num = state.get('generations_num', 0)\n",
    "\n",
    "  context = '\\n\\n'.join(map(lambda doc: doc.page_content, documents))\n",
    "  generation = rag_chain.invoke({\n",
    "    'context': context,\n",
    "    'question': question,\n",
    "    'a': choices[0],\n",
    "    'b': choices[1],\n",
    "    'c': choices[2],\n",
    "    'd': choices[3],\n",
    "  })\n",
    "\n",
    "  return {'generation': generation, 'generations_num': generations_num + 1}\n",
    "\n",
    "def grade_generation(state):\n",
    "  print('---CHECK HALLUCINATIONS---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "  generation = state['generation']\n",
    "  generations_num = state['generations_num']\n",
    "\n",
    "  if generations_num >= 2:\n",
    "    return 'useful'\n",
    "\n",
    "  try:\n",
    "    context = '\\n\\n'.join(map(lambda doc: doc.page_content, documents))\n",
    "    score = hallucination_grader.invoke({\n",
    "      'documents': context,\n",
    "      'generation': generation,\n",
    "    })\n",
    "    grade = score.binary_score\n",
    "  except:\n",
    "    grade = 'no'\n",
    "\n",
    "  if grade == 'yes':\n",
    "    print('---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---')\n",
    "    print('---GRADE GENERATION vs QUESTION---')\n",
    "\n",
    "    try:\n",
    "      score = answer_grader.invoke({'question': question,'generation': generation})\n",
    "      grade = score.binary_score\n",
    "    except:\n",
    "      grade = 'no'\n",
    "\n",
    "    if grade == 'yes':\n",
    "      print('---DECISION: GENERATION ADDRESSES QUESTION---')\n",
    "      return 'useful'\n",
    "    else:\n",
    "      print('---DECISION: GENERATION DOES NOT ADDRESS QUESTION---')\n",
    "      return 'not useful'\n",
    "  else:\n",
    "    print('---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---')\n",
    "    return 'not supported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7428621a0700>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node('determine_specialized_srcs', determine_specialized_srcs)\n",
    "\n",
    "workflow.add_node('generate_step_back_query', generate_step_back_query)\n",
    "workflow.add_node('generate_rewritten_query', generate_rewritten_query)\n",
    "workflow.add_node('generate_subqueries', generate_subqueries)\n",
    "\n",
    "workflow.add_node('generate_hyde_docs', generate_hyde_docs)\n",
    "\n",
    "workflow.add_node('vector_store_retriever', vector_store_retriever_node)\n",
    "workflow.add_node('pub_med_retriever', pub_med_retriever_node)\n",
    "workflow.add_node('arxiv_retriever', arxiv_retriever_node)\n",
    "workflow.add_node('ncbi_protein_db_retriever', ncbi_protein_db_retriever_node)\n",
    "workflow.add_node('ncbi_gene_db_retriever_node', ncbi_gene_db_retriever_node)\n",
    "\n",
    "workflow.add_node('websearch', web_search)\n",
    "workflow.add_node('generate', generate)\n",
    "workflow.add_node('grade_documents', grade_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7428621a0700>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(START, 'determine_specialized_srcs')\n",
    "workflow.add_conditional_edges(\n",
    "  'determine_specialized_srcs',\n",
    "  route_question,\n",
    "  {\n",
    "    'websearch': 'websearch',\n",
    "    'specialized_srcs': 'generate_step_back_query',\n",
    "  },\n",
    ")\n",
    "\n",
    "workflow.add_edge('generate_step_back_query', 'generate_rewritten_query')\n",
    "workflow.add_edge('generate_rewritten_query', 'generate_subqueries')\n",
    "workflow.add_edge('generate_subqueries', 'generate_hyde_docs')\n",
    "\n",
    "workflow.add_edge('generate_hyde_docs', 'vector_store_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'pub_med_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'arxiv_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'ncbi_protein_db_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'ncbi_gene_db_retriever_node')\n",
    "\n",
    "workflow.add_edge('vector_store_retriever', 'grade_documents')\n",
    "workflow.add_edge('pub_med_retriever', 'grade_documents')\n",
    "workflow.add_edge('arxiv_retriever', 'grade_documents')\n",
    "workflow.add_edge('ncbi_protein_db_retriever', 'grade_documents')\n",
    "workflow.add_edge('ncbi_gene_db_retriever_node', 'grade_documents')\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "  'grade_documents',\n",
    "  decide_to_generate,\n",
    "  {\n",
    "    'websearch': 'websearch',\n",
    "    'generate': 'generate',\n",
    "  },\n",
    ")\n",
    "workflow.add_edge('websearch', 'generate')\n",
    "workflow.add_conditional_edges(\n",
    "  'generate',\n",
    "  grade_generation,\n",
    "  {\n",
    "    'not supported': 'generate',\n",
    "    'useful': END,\n",
    "    'not useful': 'websearch',\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DETERMINE SPECIALIZED SOURCES---\n",
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO SPECIALIZED SOURCES: NCBI_PROTEIN---\n",
      "---GENERATE STEP-BACK QUERY---\n",
      "---GENERATE REWRITTEN QUERY---\n",
      "---GENERATE SUBQUERIES---\n",
      "---FINAL SUBQUERIES NUMBER: 4---\n",
      "---GENERATE HYDE DOCUMENTS---\n",
      "---RETRIEVE FROM NCBI PROTEIN DB---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---INITIAL DOCUMENTS NUMBER: 5---\n",
      "---GRADE DOCUMENT (1/5)---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE DOCUMENT (2/5)---\n",
      "---GRADE: DOCUMENT IS REPEATED---\n",
      "---GRADE DOCUMENT (3/5)---\n",
      "---GRADE: DOCUMENT IS REPEATED---\n",
      "---GRADE DOCUMENT (4/5)---\n",
      "---GRADE: DOCUMENT IS REPEATED---\n",
      "---GRADE DOCUMENT (5/5)---\n",
      "---GRADE: DOCUMENT IS REPEATED---\n",
      "---FINAL DOCUMENTS NUMBER: 0---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
      "Fusing candidates: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\n",
      "---WEB SEARCH---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
      "Fusing candidates: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK HALLUCINATIONS---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Count each amino acid in the ABW05875 sequence',\n",
       " 'specialized_srcs': ['ncbi_protein'],\n",
       " 'step_back_query': 'To generate a step-back query that improves context retrieval for the original query \"Count each amino acid in the ABW05875 sequence\", I would ask:\\n\\n\"What is the general process of analyzing protein sequences, and what types of information can be extracted from them?\"\\n\\nThis more general query aims to retrieve relevant background information on protein sequence analysis, including topics such as:\\n\\n* Sequence annotation\\n* Amino acid composition\\n* Protein structure prediction\\n* Bioinformatics tools and techniques\\n\\nBy asking this step-back query, the RAG system can provide a broader context that may include relevant information about protein sequences in general, which can then be used to improve the retrieval of specific information related to the original query.',\n",
       " 'rewritten_query': 'Here\\'s a rewritten version of the query that is more specific, detailed, and likely to retrieve relevant information:\\n\\n\"Count and list the frequency of each individual amino acid (alanine, arginine, asparagine, aspartic acid, cysteine, glutamic acid, glutamine, glycine, histidine, isoleucine, leucine, lysine, methionine, phenylalanine, proline, serine, threonine, tryptophan, tyrosine, valine) in the protein sequence ABW05875.\"\\n\\nThis rewritten query:\\n\\n* Specifies that you want to count each individual amino acid\\n* Lists all 20 standard amino acids for clarity\\n* Includes the specific protein sequence ID (ABW05875) to ensure accurate retrieval of relevant information.',\n",
       " 'subqueries': ['What are the counts of each amino acid type (A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y) in the ABW05875 sequence?',\n",
       "  'How many occurrences of each amino acid are there in the ABW05875 sequence?',\n",
       "  'What is the frequency distribution of each amino acid type in the ABW05875 sequence?',\n",
       "  'Can you provide a summary of the amino acid composition of the ABW05875 sequence?'],\n",
       " 'generated_docs': ['Here is a scientific paper-style passage answering the question:\\n\\n**Title:** Amino Acid Composition Analysis of the ABW05875 Sequence\\n\\n**Abstract:**\\nThe ABW05875 protein sequence, a putative enzyme of unknown function, was analyzed to determine its amino acid composition. This study aimed to identify and quantify each amino acid present in the sequence.\\n\\n**Results:**\\n\\nTo count each amino acid in the ABW05875 sequence, we employed a computational approach using bioinformatics tools. The sequence, comprising 317 amino acids, was subjected to analysis using the ExPASy ProtParam tool (Gasteiger et al., 2005). The results are presented in Table 1.\\n\\n| Amino Acid | Count |\\n| --- | --- |\\n| Alanine (A) | 23 |\\n| Arginine (R) | 17 |\\n| Asparagine (N) | 14 |\\n| Aspartic acid (D) | 21 |\\n| Cysteine (C) | 6 |\\n| Glutamic acid (E) | 25 |\\n| Glutamine (Q) | 12 |\\n| Glycine (G) | 15 |\\n| Histidine (H) | 8 |\\n| Isoleucine (I) | 10 |\\n| Leucine (L) | 22 |\\n| Lysine (K) | 18 |\\n| Methionine (M) | 5 |\\n| Phenylalanine (F) | 12 |\\n| Proline (P) | 9 |\\n| Serine (S) | 16 |\\n| Threonine (T) | 13 |\\n| Tryptophan (W) | 4 |\\n| Tyrosine (Y) | 7 |\\n| Valine (V) | 20 |\\n\\n**Discussion:**\\nThe amino acid composition of the ABW05875 sequence reveals a diverse distribution of residues, with no single amino acid dominating the sequence. The presence of hydrophobic and hydrophilic residues suggests that the protein may have multiple functional domains or binding sites.\\n\\n**Conclusion:**\\nThis study provides a comprehensive analysis of the amino acid composition of the ABW05875 sequence, which will be useful for further structural and functional studies of this protein.\\n\\nReferences:\\nGasteiger, E., Gattiker, A., Hoogland, C., Ivanyi, I., Appel, R. D., & Bairoch, A. (2005). ExPASy: The proteomics server for in-depth protein knowledge and analysis. Nucleic Acids Research, 33(W1), W562-W565.',\n",
       "  'Here is a scientific paper passage answering the question:\\n\\n**Improving Context Retrieval through Step-Back Querying: A Case Study on Protein Sequence Analysis**\\n\\nThe process of analyzing protein sequences involves a multifaceted approach that encompasses various bioinformatics tools and techniques. To better understand the context surrounding the original query \"Count each amino acid in the ABW05875 sequence\", we can ask a more general step-back query, such as \"What is the general process of analyzing protein sequences, and what types of information can be extracted from them?\" This query aims to retrieve relevant background information on protein sequence analysis, including topics such as sequence annotation, amino acid composition, protein structure prediction, and bioinformatics tools and techniques.\\n\\nBy posing this step-back query, we can provide a broader context that may include relevant information about protein sequences in general. This, in turn, can be used to improve the retrieval of specific information related to the original query. For instance, understanding the process of sequence annotation can inform the identification of relevant features within the ABW05875 sequence, while knowledge of amino acid composition can provide context for the counting of individual amino acids.\\n\\nThe RAG system\\'s ability to generate step-back queries and retrieve relevant background information has significant implications for improving context retrieval in bioinformatics applications. By leveraging this capability, researchers can gain a deeper understanding of the underlying processes and relationships that inform their specific research questions, ultimately leading to more accurate and informative results.',\n",
       "  'Here is a scientific paper passage answering the question:\\n\\n**Frequency Analysis of Amino Acids in the Protein Sequence ABW05875**\\n\\nThe protein sequence ABW05875, a member of the ABC transporter family, was analyzed for its amino acid composition. Using bioinformatics tools and algorithms, we counted and listed the frequency of each individual amino acid present in this protein sequence.\\n\\nAs shown in Table 1, the most abundant amino acids in ABW05875 are alanine (ALA), serine (SER), and glycine (GLY), which together account for approximately 23.4% of the total amino acid content. The least abundant amino acids are tryptophan (TRP) and tyrosine (TYR), with frequencies of only 1.2% and 1.5%, respectively.\\n\\n| Amino Acid | Frequency (%) |\\n| --- | --- |\\n| ALA | 9.8 |\\n| SER | 7.3 |\\n| GLY | 6.4 |\\n| THR | 5.6 |\\n| LEU | 5.3 |\\n| VAL | 4.9 |\\n| ILE | 4.2 |\\n| LYS | 3.9 |\\n| ARG | 3.5 |\\n| HIS | 3.1 |\\n| GLN | 2.8 |\\n| ASN | 2.6 |\\n| ASP | 2.4 |\\n| CYS | 2.1 |\\n| MET | 1.9 |\\n| PHE | 1.7 |\\n| PRO | 1.5 |\\n| TYR | 1.5 |\\n| TRP | 1.2 |\\n\\nThese results provide valuable insights into the biochemical properties and functional characteristics of ABW05875, which may be useful for further research and applications in biotechnology and medicine.\\n\\n**Table 1:** Frequency analysis of amino acids in the protein sequence ABW05875.',\n",
       "  'Here is a scientific paper passage that answers the question:\\n\\n**Title:** Amino Acid Composition Analysis of the ABW05875 Sequence\\n\\n**Abstract:**\\n\\nThe ABW05875 protein sequence has been analyzed to determine its amino acid composition. This study aimed to identify and quantify the presence of each amino acid type in the given sequence.\\n\\n**Results:**\\n\\nTo perform this analysis, we utilized a bioinformatics tool that calculates the frequency of each amino acid residue within a given protein sequence. The results are presented in Table 1 below:\\n\\n| Amino Acid | Count |\\n| --- | --- |\\n| Alanine (A) | 12 |\\n| Cysteine (C) | 5 |\\n| Aspartic Acid (D) | 8 |\\n| Glutamic Acid (E) | 15 |\\n| Phenylalanine (F) | 3 |\\n| Glycine (G) | 9 |\\n| Histidine (H) | 2 |\\n| Isoleucine (I) | 6 |\\n| Lysine (K) | 10 |\\n| Leucine (L) | 18 |\\n| Methionine (M) | 4 |\\n| Asparagine (N) | 7 |\\n| Proline (P) | 5 |\\n| Glutamine (Q) | 6 |\\n| Arginine (R) | 12 |\\n| Serine (S) | 11 |\\n| Threonine (T) | 9 |\\n| Valine (V) | 14 |\\n| Tryptophan (W) | 1 |\\n| Tyrosine (Y) | 2 |\\n\\n**Discussion:**\\n\\nThe results show that the ABW05875 sequence is rich in hydrophobic amino acids, such as Leucine and Valine, which are often found in protein structures involved in membrane interactions. The presence of charged residues like Glutamic Acid, Lysine, and Arginine suggests potential roles in protein-protein interactions or enzymatic activities.',\n",
       "  'Here is a scientific paper-style passage answering the question:\\n\\n**Title:** Amino Acid Composition Analysis of the ABW05875 Sequence\\n\\n**Abstract:**\\nThe ABW05875 protein sequence, a member of the hypothetical protein family, has been analyzed to determine its amino acid composition. This study aimed to identify and quantify the occurrences of each amino acid in the ABW05875 sequence.\\n\\n**Results:**\\n\\nTo investigate the amino acid composition of the ABW05875 sequence, we employed a comprehensive analysis using bioinformatics tools. The sequence was first retrieved from the UniProt database (UniProt ID: ABW05875) and then subjected to amino acid frequency analysis using the Sequence Analysis software package (SAAP).\\n\\nThe results of this analysis are presented in Table 1, which shows the number of occurrences for each of the 20 standard amino acids in the ABW05875 sequence.\\n\\n**Table 1:** Amino Acid Composition of the ABW05875 Sequence\\n\\n| Amino Acid | Number of Occurrences |\\n| --- | --- |\\n| Alanine (A) | 23 |\\n| Arginine (R) | 17 |\\n| Asparagine (N) | 12 |\\n| Aspartic acid (D) | 15 |\\n| Cysteine (C) | 8 |\\n| Glutamic acid (E) | 20 |\\n| Glutamine (Q) | 10 |\\n| Glycine (G) | 25 |\\n| Histidine (H) | 5 |\\n| Isoleucine (I) | 12 |\\n| Leucine (L) | 28 |\\n| Lysine (K) | 15 |\\n| Methionine (M) | 9 |\\n| Phenylalanine (F) | 18 |\\n| Proline (P) | 10 |\\n| Serine (S) | 22 |\\n| Threonine (T) | 14 |\\n| Tryptophan (W) | 4 |\\n| Tyrosine (Y) | 12 |\\n| Valine (V) | 20 |\\n\\n**Discussion:**\\nThe amino acid composition of the ABW05875 sequence reveals a diverse distribution of amino acids, with some residues being more abundant than others. The most frequent amino acids in this sequence are Leucine (L), Glycine (G), and Serine (S), which account for approximately 40% of the total amino acid content. In contrast, Tryptophan (W) and Histidine (H) are relatively rare, with only 4 and 5 occurrences, respectively.\\n\\nThese findings provide valuable insights into the biochemical properties and potential functions of the ABW05875 protein sequence. Further analysis is warranted to explore the implications of this amino acid composition on protein structure, function, and interactions.',\n",
       "  \"Here's a potential scientific paper passage that answers the question:\\n\\n**Title:** Characterization of the Amino Acid Composition of the ABW05875 Sequence\\n\\n**Abstract:**\\n\\nThe ABW05875 sequence, a protein of unknown function, has been analyzed to determine its amino acid composition. Using bioinformatics tools and statistical methods, we have calculated the frequency distribution of each amino acid type in this sequence.\\n\\n**Results:**\\n\\nTo investigate the amino acid composition of the ABW05875 sequence, we employed a sliding window approach with a window size of 100 amino acids. The resulting frequency distributions for each amino acid type are presented in Table 1 and Figure 1.\\n\\n| Amino Acid Type | Frequency (%) |\\n| --- | --- |\\n| Alanine (A) | 12.5 ± 2.1 |\\n| Arginine (R) | 8.3 ± 1.4 |\\n| Asparagine (N) | 9.7 ± 1.6 |\\n| Aspartic Acid (D) | 10.4 ± 1.7 |\\n| Cysteine (C) | 2.5 ± 0.6 |\\n| Glutamic Acid (E) | 12.8 ± 2.2 |\\n| Glutamine (Q) | 9.3 ± 1.5 |\\n| Glycine (G) | 15.4 ± 2.5 |\\n| Histidine (H) | 6.7 ± 1.2 |\\n| Isoleucine (I) | 8.1 ± 1.4 |\\n| Leucine (L) | 11.9 ± 2.0 |\\n| Lysine (K) | 10.5 ± 1.7 |\\n| Methionine (M) | 3.6 ± 0.8 |\\n| Phenylalanine (F) | 4.9 ± 1.0 |\\n| Proline (P) | 5.2 ± 1.1 |\\n| Serine (S) | 11.7 ± 2.0 |\\n| Threonine (T) | 10.3 ± 1.7 |\\n| Tryptophan (W) | 2.8 ± 0.6 |\\n| Tyrosine (Y) | 4.5 ± 1.0 |\\n| Valine (V) | 9.9 ± 1.6 |\\n\\n**Discussion:**\\n\\nThe frequency distribution of each amino acid type in the ABW05875 sequence reveals a relatively balanced composition, with no single amino acid dominating the others. The presence of a high frequency of Glycine (G), Leucine (L), and Glutamic Acid (E) suggests that these residues may play important structural or functional roles in this protein. In contrast, the lower frequencies of Cysteine (C), Methionine (M), and Tryptophan (W) suggest that these residues may be less critical for the overall structure or function of the ABW05875 sequence.\\n\\n**Conclusion:**\\n\\nIn conclusion, our analysis has provided a detailed characterization of the amino acid composition of the ABW05875 sequence. The frequency distribution of each amino acid type in this sequence will serve as a valuable resource for further studies on protein structure and function.\",\n",
       "  \"Here is a potential scientific paper passage that answers the question:\\n\\n**Title:** Characterization of the Amino Acid Composition of the ABW05875 Sequence\\n\\n**Abstract:**\\n\\nThe ABW05875 sequence, a protein of unknown function, has been identified through various bioinformatic analyses. To gain insight into its biochemical properties, we undertook a comprehensive analysis of its amino acid composition. Using a combination of computational tools and manual annotation, we determined the amino acid sequence of ABW05875 to be:\\n\\nMVLSPADKTNVKAAAGDVGSNLCVDPVNFKLLSHCLLVTLAHLPAEFTPAVDGVYAAP\\nGDGSGNYPDIASLVSHPAQDYPEAPTLVRDGNFSCAVTSAVSDLGNTATPDLVEWGP\\nDAVLVPVTIGQAPAGSVAQSALSDHAPEPVSGSQEGSFGSGSYSGTNQVKAHNLP\\nSLAEDPGYVGGGDTPGSPASVSGGAAPAGSAPAAPDPDPPAAPADTAPAAPAAPAAP\\n\\n**Results:**\\n\\nThe amino acid composition of the ABW05875 sequence was analyzed using a variety of computational tools, including the Expasy ProtParam tool and the MASCOT protein identification software. The results are summarized in Table 1.\\n\\n| Amino Acid | Frequency |\\n| --- | --- |\\n| Alanine (A) | 12.5% |\\n| Arginine (R) | 8.3% |\\n| Asparagine (N) | 6.2% |\\n| Aspartic acid (D) | 9.4% |\\n| Cysteine (C) | 1.9% |\\n| Glutamic acid (E) | 11.5% |\\n| Glutamine (Q) | 7.3% |\\n| Glycine (G) | 10.8% |\\n| Histidine (H) | 2.4% |\\n| Isoleucine (I) | 6.9% |\\n| Leucine (L) | 14.1% |\\n| Lysine (K) | 7.5% |\\n| Methionine (M) | 3.8% |\\n| Phenylalanine (F) | 4.2% |\\n| Proline (P) | 6.9% |\\n| Serine (S) | 10.3% |\\n| Threonine (T) | 7.5% |\\n| Tryptophan (W) | 1.9% |\\n| Tyrosine (Y) | 4.2% |\\n\\n**Discussion:**\\n\\nThe amino acid composition of the ABW05875 sequence reveals a high frequency of hydrophobic residues, including leucine and glycine, which may contribute to its structural properties. The presence of charged residues, such as arginine and glutamic acid, suggests that the protein may have electrostatic interactions with other molecules. Further analysis is needed to determine the functional significance of these amino acids in the context of ABW05875's biochemical properties.\"],\n",
       " 'documents': [Document(metadata={'source': 'https://uk.mathworks.com/help/bioinfo/ref/aacount.html'}, page_content='Count amino acids in a sequence [`countStruct`](https://uk.mathworks.com/help/bioinfo/ref/aacount.html#mw_404fc65d-c6ba-4983-b689-1d2350c7a4ba) = aacount([`SeqAA`](https://uk.mathworks.com/help/bioinfo/ref/aacount.html#mw_a6707376-a71b-4224-a43e-04d4bec71590)) counts the number of each type of amino acid in SeqAA, an amino acid sequence, and returns the counts in countStruct, a 1-by-1 MATLAB® structure containing fields for the standard 20 amino acids (A, R, N, D, C, Q, E, G, H, I, L, K, M, F, P, S, T, W, Y, and V). For example, countStruct = aacount(SeqAA,Chart=\"pie\") creates a pie chart showing relative proportions of the amino acids. Count the amino acids in the sequence, and display the results in a pie chart. Ambiguous amino acid characters (B, Z, or X), gaps that are indicated by a hyphens (-), and end terminators (*) are ignored by default.'),\n",
       "  Document(metadata={'source': 'https://stackoverflow.com/questions/5774099/defining-a-function-that-counts-relative-frequency-of-amino-acids'}, page_content='use defaultdict. from collections import defaultdict mydict = defaultdict(int) for aa in mysecuence: mydict[aa] +=1 This works for aminoacids (proteins).'),\n",
       "  Document(metadata={'source': 'https://www.chegg.com/homework-help/questions-and-answers/want-count-number-occurrences-amino-acid-protein-sequence-write-function-aminoacidcounts-t-q91491852'}, page_content='Question: You want to count the number of occurrences of each amino acid in the protein sequence: Write a function, aminoacid_counts, which takes one argument: 1. A string, which represents the protein sequence. The function must return: • A dictionary in which keys are the different amino acids in the sequence.'),\n",
       "  Document(metadata={'source': 'https://www.biochemithon.in/biology/bioinformatics/how-to-count-amino-acid-in-a-protein-sequence-using-python-program/'}, page_content=\"How to count amino acid in a protein sequence using Python program - BioChemiThon How to count amino acid in a protein sequence using Python program How to count amino acid in a protein sequence using Python program How to count amino acid in a protein sequence using Python program In this article, we are going to learn how python can be useful in finding amino acid in a given protein sequence. Here, we will learn how can we find the total length of a sequence and find the number of the specific amino acid in a sequence. total_amino_acid = len(given_sequence) find count of 'M' in the given sequence Save my name, email, and website in this browser for the next time I comment.\"),\n",
       "  Document(metadata={'source': 'https://www.mathworks.com/help/bioinfo/ref/aacount.html'}, page_content='Count amino acids in sequence counts the number of each type of amino acid in SeqAA, an amino acid sequence, and returns the counts in countStruct, a 1-by-1 MATLABÂ® structure containing fields for the standard 20 amino acids Count amino acids in a sequence Count the amino acids in the sequence, and display the results in a pie chart. SeqAA â\\x80\\x94 Amino acid sequence character vector | string scalar | row vector of positive integers | structure Amino acid sequence, specified as one of the following. Row vector of integers specifying an amino acid sequence. Structure that contains an amino acid sequence in the countStruct â\\x80\\x94 Total count of amino acids structure Total amino acid count, returned as a structure.'),\n",
       "  Document(metadata={'source': 'https://uk.mathworks.com/help/bioinfo/ref/aacount.html'}, page_content='Count amino acids in a sequence [`countStruct`](https://uk.mathworks.com/help/bioinfo/ref/aacount.html#mw_404fc65d-c6ba-4983-b689-1d2350c7a4ba) = aacount([`SeqAA`](https://uk.mathworks.com/help/bioinfo/ref/aacount.html#mw_a6707376-a71b-4224-a43e-04d4bec71590)) counts the number of each type of amino acid in SeqAA, an amino acid sequence, and returns the counts in countStruct, a 1-by-1 MATLAB® structure containing fields for the standard 20 amino acids (A, R, N, D, C, Q, E, G, H, I, L, K, M, F, P, S, T, W, Y, and V). For example, countStruct = aacount(SeqAA,Chart=\"pie\") creates a pie chart showing relative proportions of the amino acids. Count the amino acids in the sequence, and display the results in a pie chart. Ambiguous amino acid characters (B, Z, or X), gaps that are indicated by a hyphens (-), and end terminators (*) are ignored by default.'),\n",
       "  Document(metadata={'source': 'https://www.biochemithon.in/biology/bioinformatics/how-to-count-amino-acid-in-a-protein-sequence-using-python-program/'}, page_content=\"How to count amino acid in a protein sequence using Python program - BioChemiThon How to count amino acid in a protein sequence using Python program How to count amino acid in a protein sequence using Python program How to count amino acid in a protein sequence using Python program In this article, we are going to learn how python can be useful in finding amino acid in a given protein sequence. Here, we will learn how can we find the total length of a sequence and find the number of the specific amino acid in a sequence. total_amino_acid = len(given_sequence) find count of 'M' in the given sequence Save my name, email, and website in this browser for the next time I comment.\"),\n",
       "  Document(metadata={'source': 'https://www.mathworks.com/help/bioinfo/ref/aacount.html'}, page_content='Count amino acids in sequence counts the number of each type of amino acid in SeqAA, an amino acid sequence, and returns the counts in countStruct, a 1-by-1 MATLABÂ® structure containing fields for the standard 20 amino acids Count amino acids in a sequence Count the amino acids in the sequence, and display the results in a pie chart. SeqAA â\\x80\\x94 Amino acid sequence character vector | string scalar | row vector of positive integers | structure Amino acid sequence, specified as one of the following. Row vector of integers specifying an amino acid sequence. Structure that contains an amino acid sequence in the countStruct â\\x80\\x94 Total count of amino acids structure Total amino acid count, returned as a structure.'),\n",
       "  Document(metadata={'source': 'https://www.mathworks.com/help/bioinfo/ref/atomiccomp.html'}, page_content='SeqAA. Amino acid sequence. Enter a character vector or vector of integers from the table Mapping Amino Acid Letter Codes to Integers.You can also enter a structure with the field Sequence.'),\n",
       "  Document(metadata={'source': 'https://www.uniprot.org/help/sequences'}, page_content='For the amino acids selenocysteine (Sec; U) and pyrrolysine (Pyl; O), we follow the proposed nomenclature. For each isoform, the name of the isoform is provided, as well as its length and molecular mass in Daltons. The mass is calculated on the basis of the amino acid composition of the entire sequence.')],\n",
       " 'web_search': 'Yes',\n",
       " 'generation': \"I'm sorry, but as an AI language model, I don't have access to the ABW05875 sequence. Could you please provide me with the sequence so that I can count each amino acid?\",\n",
       " 'generations_num': 2}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({'question': 'Count each amino acid in the ABW05875 sequence'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the afferent cranial nerve nuclei?</td>\n",
       "      <td>Trigeminal sensory nucleus- fibres carry gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the order of the cranial nerves ?</td>\n",
       "      <td>1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the efferent cranial nerve nuclei?</td>\n",
       "      <td>Edinger-westphal nucleus\\nOculomotor nucleus\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which nuclei share the embryo logical origin -...</td>\n",
       "      <td>Oculomotor nucleus Trochlear nucleus Abducens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which nuclei share the embryo logical origin- ...</td>\n",
       "      <td>Trigeminal motor nucleus Facial motor nucleus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>What are the functions each of these structure...</td>\n",
       "      <td>Cortex is involved in perception Hypothalamus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>What are the stimuli for olfaction ?</td>\n",
       "      <td>Airborne molecules \\nAlcohols \\nEsters \\nAroma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>What percentage of human genome are involved w...</td>\n",
       "      <td>3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>What is the difference between threshold value...</td>\n",
       "      <td>Lipid soluble have low thresholds while water ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>What are the 3 main cell types involved in olf...</td>\n",
       "      <td>Olfactory receptors - site of transduction Sup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "0          What are the afferent cranial nerve nuclei?   \n",
       "1            What is the order of the cranial nerves ?   \n",
       "2          What are the efferent cranial nerve nuclei?   \n",
       "3    Which nuclei share the embryo logical origin -...   \n",
       "4    Which nuclei share the embryo logical origin- ...   \n",
       "..                                                 ...   \n",
       "495  What are the functions each of these structure...   \n",
       "496               What are the stimuli for olfaction ?   \n",
       "497  What percentage of human genome are involved w...   \n",
       "498  What is the difference between threshold value...   \n",
       "499  What are the 3 main cell types involved in olf...   \n",
       "\n",
       "                                                answer  \n",
       "0    Trigeminal sensory nucleus- fibres carry gener...  \n",
       "1    1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...  \n",
       "2    Edinger-westphal nucleus\\nOculomotor nucleus\\n...  \n",
       "3    Oculomotor nucleus Trochlear nucleus Abducens ...  \n",
       "4    Trigeminal motor nucleus Facial motor nucleus ...  \n",
       "..                                                 ...  \n",
       "495  Cortex is involved in perception Hypothalamus ...  \n",
       "496  Airborne molecules \\nAlcohols \\nEsters \\nAroma...  \n",
       "497                                                 3%  \n",
       "498  Lipid soluble have low thresholds while water ...  \n",
       "499  Olfactory receptors - site of transduction Sup...  \n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "letter_to_number = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
    "\n",
    "def eval_rag(model: object, subset: str) -> float:\n",
    "  dataset = load_dataset('cais/mmlu', subset)\n",
    "  test_df = dataset['test'].to_pandas()\n",
    "\n",
    "  correct_answers_count = 0\n",
    "\n",
    "  for index, row in tqdm(list(test_df.iterrows()), desc='Questions'):\n",
    "    question = row['question']\n",
    "    choices = row['choices']\n",
    "    correct_answer = row['answer']\n",
    "\n",
    "    llm_answer = app.invoke({'question': question, 'choices': choices})\n",
    "    json_string = extract_json(llm_answer['generation'])\n",
    "    response_object = rag_parser.invoke(json_string)\n",
    "    print(response_object)\n",
    "\n",
    "    llm_answer_letter = llm_answer.correct_answer.strip().lower()[0]\n",
    "\n",
    "    if llm_answer_letter not in letter_to_number:\n",
    "      continue\n",
    "\n",
    "    llm_answer_num = letter_to_number[llm_answer_letter]\n",
    "\n",
    "    if llm_answer_num == correct_answer:\n",
    "      correct_answers_count += 1\n",
    "\n",
    "  return correct_answers_count / len(test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
