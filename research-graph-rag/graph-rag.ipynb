{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: unidecode in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (1.3.8)\n",
      "Requirement already satisfied: scikit-learn in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (4.66.5)\n",
      "Requirement already satisfied: llm-blender in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: rouge-score in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: xmltodict in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.14.2)\n",
      "Requirement already satisfied: arxiv in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (2.1.3)\n",
      "Requirement already satisfied: click in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: transformers in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (4.45.2)\n",
      "Requirement already satisfied: torch in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (2.5.0)\n",
      "Requirement already satisfied: accelerate in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (1.0.1)\n",
      "Requirement already satisfied: safetensors in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (0.4.5)\n",
      "Requirement already satisfied: dataclasses-json in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (0.6.7)\n",
      "Requirement already satisfied: sentencepiece in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (4.25.5)\n",
      "Requirement already satisfied: absl-py in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (23.2)\n",
      "Requirement already satisfied: psutil in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (0.26.1)\n",
      "Requirement already satisfied: filelock in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from sympy==1.13.1->torch->llm-blender) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json->llm-blender) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json->llm-blender) (0.9.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from transformers->llm-blender) (0.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->llm-blender) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from jinja2->torch->llm-blender) (3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-core in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain-community in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain_experimental in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-openai in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.2.4)\n",
      "Requirement already satisfied: langchain-chroma in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: langchain_mistralai in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: langgraph in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.2.39)\n",
      "Requirement already satisfied: langchainhub in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.1.21)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-community) (2.6.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-openai) (1.52.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-chroma) (0.5.15)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-chroma) (0.115.3)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain_mistralai) (0.27.2)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain_mistralai) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain_mistralai) (0.20.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langgraph) (2.0.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.32 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langgraph) (0.1.33)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchainhub) (2.32.0.20241016)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.7.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.32.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.7.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.66.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (31.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.10)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.9.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.41.0)\n",
      "Requirement already satisfied: anyio in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.6)\n",
      "Requirement already satisfied: idna in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.26.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.2.2)\n",
      "Requirement already satisfied: pyproject_hooks in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.4.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.2)\n",
      "Requirement already satisfied: filelock in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.35.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.25.5)\n",
      "Requirement already satisfied: sympy in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (75.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from importlib-resources->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk numpy pandas unidecode scikit-learn tqdm llm-blender rouge-score xmltodict arxiv\n",
    "! pip install langchain langchain-core langchain-community langchain_experimental langchain-openai langchain-chroma langchain_mistralai langgraph langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import llm_blender\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.llms import Ollama\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import RetryOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import PubMedRetriever, ArxivRetriever\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment variables\n",
    "\n",
    "You have to define the following environment variables in the `.env` file, terminal environment, or input field within this Jupyter notebook:\n",
    "1. MISTRAL_API_KEY\n",
    "2. OPENAI_API_KEY\n",
    "3. OPENAI_PROXY\n",
    "4. TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_variables = [\n",
    "  'MISTRAL_API_KEY',\n",
    "  'OPENAI_API_KEY',\n",
    "  'OPENAI_PROXY',\n",
    "  'TAVILY_API_KEY',\n",
    "]\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "for key in env_variables:\n",
    "  value = os.getenv(key)\n",
    "\n",
    "  if value is None:\n",
    "    value = getpass(key)\n",
    "\n",
    "  os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK dictionaries\n",
    "\n",
    "These dictionaries are needed for further text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ids = [\n",
    "  'punkt_tab',\n",
    "  'punkt',\n",
    "  'stopwords',\n",
    "  'wordnet',\n",
    "]\n",
    "\n",
    "for dict_id in dict_ids:\n",
    "  nltk.download(dict_id, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Define a function for text preprocessing, which is an important step before calculating any metrics. This preprocessing function will help in cleaning the text data, making it ready for further analysis. The preprocessing involves several steps:\n",
    "1. Lowercasing\n",
    "2. Stopwords removal\n",
    "3. Lemmatization\n",
    "4. Remove accents from characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(corpus: str) -> str:\n",
    "  corpus = corpus.lower()\n",
    "  stopset = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('russian') + list(string.punctuation)\n",
    "  tokens = nltk.word_tokenize(corpus)\n",
    "  tokens = [t for t in tokens if t not in stopset]\n",
    "  tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "  corpus = ' '.join(tokens)\n",
    "  corpus = unidecode(corpus)\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Initialization\n",
    "\n",
    "Here we are initializing the Llama 3 embeddings model. The `OllamaEmbeddings` class is a component of the Ollama library, a set of pre-trained language models. This model is capable of embedding corpora of any length into a 4096-dimensional vector.\n",
    "\n",
    "The use of `OllamaEmbeddings` requires the installation of a local Ollama server, which can be found at https://ollama.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama3.1')\n",
    "store = LocalFileStore(\"./.embeddings_cache\")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "  embeddings,\n",
    "  store,\n",
    "  namespace=embeddings.model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average embeddings cosine similarity metric\n",
    "\n",
    "This function calculates the average cosine similarity between expected answers and LLM predicted answers using their respective embeddings. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them:\n",
    "\n",
    "$$\n",
    "K(a, b) = \\frac{\\sum \\limits_{i=1}^n a_i b_i}{\\sqrt{\\sum \\limits_{i=1}^n a_i^2} \\cdot \\sqrt{\\sum \\limits_{i=1}^n b_i^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_cosine_sim_metric(expected_answers: list[str], predicted_answers: list[str]) -> float:\n",
    "  results = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    expected_embedding = np.array(cached_embeddings.embed_query(expected_answer))\n",
    "    predicted_embedding = np.array(cached_embeddings.embed_query(predicted_answer))\n",
    "\n",
    "    sim = cosine_similarity(\n",
    "      expected_embedding.reshape(1, -1),\n",
    "      predicted_embedding.reshape(1, -1),\n",
    "    )[0][0]\n",
    "\n",
    "    results.append(sim)\n",
    "\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothie_f = nltk.translate.bleu_score.SmoothingFunction().method4\n",
    "\n",
    "def bleu_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    predicted_tokens = nltk.word_tokenize(predicted_answer)\n",
    "    expected_tokens = [nltk.word_tokenize(expected_answer)]\n",
    "\n",
    "    score = nltk.translate.bleu_score.sentence_bleu(\n",
    "      expected_tokens,\n",
    "      predicted_tokens,\n",
    "      smoothing_function=smoothie_f,\n",
    "    )\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_1_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rogue_1_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_1_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rouge1'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_l_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def rogue_l_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_l_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rougeL'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = Path('./docs')\n",
    "docs_cache_dir = Path('./.docs_cache')\n",
    "raw_docs_pkl_path = docs_cache_dir / 'parsed_docs_cache.pkl'\n",
    "\n",
    "docs = None\n",
    "\n",
    "if os.path.exists(raw_docs_pkl_path):\n",
    "  with open(raw_docs_pkl_path, 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "else:\n",
    "  docs = []\n",
    "  for file in docs_dir.iterdir():\n",
    "    docs.extend(PDFMinerLoader(file).load())\n",
    "\n",
    "  with open(raw_docs_pkl_path, 'wb') as f:\n",
    "    pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17443"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_docs_pkl_path = docs_cache_dir / 'splitted_docs_cache.pkl'\n",
    "\n",
    "if os.path.exists(splitted_docs_pkl_path):\n",
    "  with open(splitted_docs_pkl_path, 'rb') as f:\n",
    "    splitted_docs = pickle.load(f)\n",
    "else:\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=750,\n",
    "    chunk_overlap=250,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "  )\n",
    "  splitted_docs = text_splitter.create_documents([doc.page_content for doc in docs])\n",
    "\n",
    "  with open(splitted_docs_pkl_path, 'wb') as f:\n",
    "    pickle.dump(splitted_docs, f)\n",
    "\n",
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "  embedding_function=cached_embeddings,\n",
    "  persist_directory='./chroma_db'\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define JSON extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(response):\n",
    "  json_pattern = r'\\{.*?\\}'\n",
    "  match = re.search(json_pattern, response, re.DOTALL)\n",
    "\n",
    "  if match:\n",
    "    return match.group().strip()\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='llama3.1', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sources=[]\n",
      "sources=['vectorstore']\n"
     ]
    }
   ],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "  sources: List[str] = Field(\n",
    "    description='Given a user question select the retrieval methods you consider the most appropriate for addressing this question. You may also return an empty array if no methods are required.',\n",
    "  )\n",
    "\n",
    "route_parser = PydanticOutputParser(pydantic_object=RouteQuery)\n",
    "route_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=route_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "route_template = \"\"\"\n",
    "You are an expert at selecting retrieval methods.\n",
    "Given a user question select the retrieval methods you consider the most appropriate for addressing user question.\n",
    "You may also return an empty array if no methods are required.\n",
    "\n",
    "Possible retrieval methods:\n",
    "1. The \"vectorstore\" retriever contains documents related to neurobiology and medicine. Use the vectorstore for questions on these topics.\n",
    "2. The \"pubmed\" retriever contains biomedical literature and research articles. It is particularly useful for answering detailed questions about medical research, clinical studies, and scientific discoveries.\n",
    "3. The \"arxiv\" retriever contains preprints of research papers across various scientific fields, including physics, mathematics, computer science, and biology. Use the arxiv for questions on recent scientific research and theoretical studies in these areas.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\"\"\"\n",
    "route_prompt = PromptTemplate(\n",
    "  template=route_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': route_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "question_router = RunnableParallel(\n",
    "  completion=route_prompt | llm | extract_json, prompt_value=route_prompt\n",
    ") | RunnableLambda(lambda x: route_retry_parser.parse_with_prompt(**x))\n",
    "print(question_router.invoke({'question': 'Who will the Bears draft first in the NFL draft?'}))\n",
    "print(question_router.invoke({'question': 'What are the functions of the oculomotor nerve?'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade documents chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeDocuments(binary_score='yes')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "  binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "docs_grade_parser = PydanticOutputParser(pydantic_object=GradeDocuments)\n",
    "docs_grade_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=docs_grade_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "docs_grade_template = \"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Retrieved document:\n",
    "{document}\n",
    "\"\"\"\n",
    "docs_grade_prompt = PromptTemplate(\n",
    "  template=docs_grade_template,\n",
    "  input_variables=['document', 'question'],\n",
    "  partial_variables={'format_instructions': docs_grade_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "docs_grade_grader = RunnableParallel(\n",
    "  completion=docs_grade_prompt | llm | extract_json, prompt_value=docs_grade_prompt\n",
    ") | RunnableLambda(lambda x: docs_grade_retry_parser.parse_with_prompt(**x))\n",
    "docs_grade_grader.invoke({'question': 'What is the color of the sky?', 'document': 'The color of the sky is blue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "  binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "hallucination_parser = PydanticOutputParser(pydantic_object=GradeHallucinations)\n",
    "hallucination_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=hallucination_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "hallucination_template = \"\"\"\n",
    "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Set of facts:\n",
    "{documents}\n",
    "\n",
    "LLM generation:\n",
    "{generation}\n",
    "\"\"\"\n",
    "hallucination_prompt = PromptTemplate(\n",
    "  template=hallucination_template,\n",
    "  input_variables=['question', 'generation'],\n",
    "  partial_variables={'format_instructions': hallucination_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "hallucination_grader = RunnableParallel(\n",
    "  completion=hallucination_prompt | llm | extract_json, prompt_value=hallucination_prompt\n",
    ") | RunnableLambda(lambda x: hallucination_retry_parser.parse_with_prompt(**x))\n",
    "print(hallucination_grader.invoke({'documents': ['Sky is blue'], 'generation': 'The color of the sky is blue'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer grade chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "  binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_parser = PydanticOutputParser(pydantic_object=GradeAnswer)\n",
    "grade_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=grade_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "grade_template = \"\"\"\n",
    "You are a grader assessing whether an answer addresses / resolves a question. \\n\n",
    "Give a binary score 'yes' or 'no'. 'yes' means that the answer resolves the question.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "LLM generation:\n",
    "{generation}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "grade_prompt = PromptTemplate(\n",
    "  template=grade_template,\n",
    "  input_variables=['question', 'generation'],\n",
    "  partial_variables={'format_instructions': grade_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "answer_grader = RunnableParallel(\n",
    "  completion=grade_prompt | llm | extract_json, prompt_value=grade_prompt\n",
    ") | RunnableLambda(lambda x: grade_retry_parser.parse_with_prompt(**x))\n",
    "print(answer_grader.invoke({\"question\": \"What is the order of the cranial nerves?\", 'generation': 'I do not know.'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a scientific paper-style passage answering the question:\\n\\n**Title:** The Cranial Nerve Plexus: A Review of the Anatomical and Neurological Organization\\n\\n**Abstract:**\\n\\nThe cranial nerves, a complex network of 12 pairs of nerves that arise from the brainstem, play a crucial role in regulating various physiological functions. Understanding their organization is essential for appreciating the intricate relationships between the central nervous system and the peripheral nervous system. This review aims to provide an overview of the order of the cranial nerves, highlighting their anatomical and neurological characteristics.\\n\\n**Introduction:**\\n\\nThe cranial nerves are a vital component of the human nervous system, responsible for controlling various functions such as sensation, movement, and autonomic regulation. The 12 pairs of cranial nerves arise from the brainstem, which is divided into three main parts: the midbrain, pons, and medulla oblongata. Each pair of cranial nerves has a distinct origin, course, and termination, reflecting their specialized functions.\\n\\n**The Order of the Cranial Nerves:**\\n\\nThe order of the cranial nerves, as traditionally described, is as follows:\\n\\n1. **Olfactory nerve (I)**: The first cranial nerve to arise from the forebrain, responsible for transmitting sensory information related to smell.\\n2. **Optic nerve (II)**: The second cranial nerve, which conveys visual information from the retina to the brain.\\n3. **Occulomotor nerve (III)**: The third cranial nerve, controlling eye movement and pupil constriction.\\n4. **Trochlear nerve (IV)**: The fourth cranial nerve, responsible for innervating the superior oblique muscle of the eye.\\n5. **Trigeminal nerve (V)**: A complex nerve with three branches (ophthalmic, maxillary, and mandibular), involved in sensation and motor control of the face.\\n6. **Abducens nerve (VI)**: The sixth cranial nerve, responsible for controlling lateral eye movement.\\n7. **Facial nerve (VII)**: The seventh cranial nerve, which regulates facial expression, taste, and hearing.\\n8. **Auditory (vestibulocochlear) nerve (VIII)**: A paired nerve responsible for transmitting auditory and vestibular information from the inner ear to the brain.\\n9. **Glossopharyngeal nerve (IX)**: The ninth cranial nerve, involved in swallowing, taste, and regulation of salivation.\\n10. **Vagus nerve (X)**: A complex nerve with multiple branches, responsible for regulating various autonomic functions, including heart rate, digestion, and respiration.\\n11. **Spinal accessory nerve (XI)**: The eleventh cranial nerve, which innervates the sternocleidomastoid and trapezius muscles.\\n12. **Hypoglossal nerve (XII)**: The twelfth and final cranial nerve, responsible for controlling tongue movement.\\n\\n**Conclusion:**\\n\\nIn conclusion, the order of the cranial nerves is a well-established concept in neuroanatomy, reflecting their distinct origins, courses, and functions. Understanding this organization is essential for appreciating the complex relationships between the central nervous system and the peripheral nervous system.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_template = \"\"\"\n",
    "Please write a scientific paper passage to answer the question\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Passage:\n",
    "\"\"\"\n",
    "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "hyde_chain.invoke({\"question\": 'What is the order of the cranial nerves ?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of a neurological syndrome?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_template = \"\"\"\n",
    "You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Step-back query:\n",
    "\"\"\"\n",
    "step_back_prompt = ChatPromptTemplate.from_template(step_back_template)\n",
    "step_back_chain = step_back_prompt | llm | StrOutputParser()\n",
    "\n",
    "step_back_chain.invoke({\"question\": 'What is Benedicts syndrome?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a rewritten query that\\'s more specific, detailed, and likely to retrieve relevant information:\\n\\n\"What is the correct anatomical order of the 12 pairs of cranial nerves, including their corresponding numbers (I-XII), functions, and any notable associations with specific brain regions or structures?\"\\n\\nThis revised query adds more specificity by:\\n\\n* Specifying the number of cranial nerves (12)\\n* Including their corresponding numbers (I-XII)\\n* Mentioning their functions\\n* Asking about associations with specific brain regions or structures\\n\\nBy doing so, it\\'s more likely to retrieve relevant information from a RAG system that can provide detailed and accurate answers.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_query_template = \"\"\"\n",
    "You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Rewritten query:\n",
    "\"\"\"\n",
    "rewrite_query_prompt = ChatPromptTemplate.from_template(rewrite_query_template)\n",
    "rewrite_query_chain = rewrite_query_prompt | llm | StrOutputParser()\n",
    "\n",
    "rewrite_query_chain.invoke({\"question\": 'What is the order of the cranial nerves?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subqueries=[\"What are the symptoms of Benedict's syndrome?\", \"What are the causes of Benedict's syndrome?\", \"How is Benedict's syndrome diagnosed?\", \"What are the treatment options for Benedict's syndrome?\"]\n"
     ]
    }
   ],
   "source": [
    "class DecompositionAnswer(BaseModel):\n",
    "  subqueries: List[str] = Field(description=\"Given the original query, decompose it into 2-4 simpler sub-queries as json array of strings\")\n",
    "\n",
    "decomposition_parser = PydanticOutputParser(pydantic_object=DecompositionAnswer)\n",
    "decomposition_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=decomposition_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "decomposition_template = \"\"\"\n",
    "You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "decomposition_prompt = PromptTemplate(\n",
    "  template=decomposition_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': decomposition_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "decomposition_chain = RunnableParallel(\n",
    "  completion=decomposition_prompt | llm | extract_json, prompt_value=decomposition_prompt\n",
    ") | RunnableLambda(lambda x: decomposition_retry_parser.parse_with_prompt(**x))\n",
    "print(decomposition_chain.invoke({\"question\": \"What is Benedicts syndrome?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /home/super-pc2/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    }
   ],
   "source": [
    "blender = llm_blender.Blender()\n",
    "blender.loadranker('llm-blender/PairRM', device='cuda')\n",
    "blender.loadfuser('llm-blender/gen_fuser_3b', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-276c93b9-fe2d-43ff-89e8-139f5c3d7479-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "gpt_llm.invoke('Hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|| 1/1 [00:00<00:00,  3.29it/s]\n",
      "Fusing candidates:   0%|          | 0/1 [00:00<?, ?it/s]2024-11-22 21:26:34.968989: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 21:26:35.056584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-22 21:26:35.091157: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-22 21:26:35.101597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 21:26:35.165866: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-22 21:26:35.664890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Fusing candidates: 100%|| 1/1 [00:02<00:00,  2.89s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The order of the cranial nerves is as follows: I. Olfactory II. Optic III. Oculomotor IV. Trochlear V. Trigeminal VI. Abducens VII. Facial VIII. Auditory (or vestibulocochlear) IX. Glossopharyngeal X. Vagus XI. Spinal accessory XII. Hypoglossal'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "llama_llm = Ollama(model='llama3.1', temperature=0)\n",
    "mistral_llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "gpt_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "llama_chain = prompt | llama_llm | StrOutputParser()\n",
    "mistral_chain = prompt | mistral_llm | StrOutputParser()\n",
    "gpt_chain = prompt | gpt_llm | StrOutputParser()\n",
    "\n",
    "def fuse_generations(dict):\n",
    "  question = dict['question']\n",
    "\n",
    "  llama_res = dict['llama_res']\n",
    "  mistral_res = dict['mistral_res']\n",
    "  gpt_res = dict['gpt_res']\n",
    "  answers = [llama_res, mistral_res, gpt_res]\n",
    "\n",
    "  fuse_generations, ranks = blender.rank_and_fuse(\n",
    "    [question],\n",
    "    [answers],\n",
    "    instructions=[''],\n",
    "    return_scores=False,\n",
    "    batch_size=2,\n",
    "    top_k=3\n",
    "  )\n",
    "  return fuse_generations[0]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "  {\n",
    "    'llama_res': llama_chain,\n",
    "    'mistral_res': mistral_chain,\n",
    "    'gpt_res': gpt_chain,\n",
    "    'question': itemgetter('question')\n",
    "  }\n",
    "  | RunnableLambda(fuse_generations)\n",
    ")\n",
    "\n",
    "rag_chain.invoke({\"context\": '', \"question\": 'What is the order of the cranial nerves?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = TavilySearchResults(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_med_retriever = PubMedRetriever()\n",
    "pub_med_retriever.invoke('What is the order of the cranial nerves?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/1912.10601v2', 'Published': datetime.date(2021, 3, 13), 'Title': 'Optimized Cranial Bandeau Remodeling', 'Authors': 'James Drake, Marina Drygala, Ricardo Fukasawa, Jochen Koenemann, Andre Linhares, Thomas Looi, John Phillips, David Qian, Nikoo Saber, Justin Toth, Chris Woodbeck, Jessie Yeung'}, page_content=\"Craniosynostosis, a condition affecting 1 in 2000 infants, is caused by\\npremature fusing of cranial vault sutures, and manifests itself in abnormal\\nskull growth patterns. Left untreated, the condition may lead to severe\\ndevelopmental impairment. Standard practice is to apply corrective cranial\\nbandeau remodeling surgery in the first year of the infant's life. The most\\nfrequent type of surgery involves the removal of the so-called fronto-orbital\\nbar from the patient's forehead and the cutting of well-placed incisions to\\nreshape the skull in order to obtain the desired result. In this paper, we\\npropose a precise optimization model for the above cranial bandeau remodeling\\nproblem and its variants. We have developed efficient algorithms that solve\\nbest incision placement, and show hardness for more general cases in the class.\\nTo the best of our knowledge this paper is the first to introduce optimization\\nmodels for craniofacial surgery applications.\"),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/1706.07649v1', 'Published': datetime.date(2017, 6, 23), 'Title': 'Computer-aided implant design for the restoration of cranial defects', 'Authors': 'Xiaojun Chen, Lu Xu, Xing Li, Jan Egger'}, page_content='Patient-specific cranial implants are important and necessary in the surgery\\nof cranial defect restoration. However, traditional methods of manual design of\\ncranial implants are complicated and time-consuming. Our purpose is to develop\\na novel software named EasyCrania to design the cranial implants conveniently\\nand efficiently. The process can be divided into five steps, which are\\nmirroring model, clipping surface, surface fitting, the generation of the\\ninitial implant and the generation of the final implant. The main concept of\\nour method is to use the geometry information of the mirrored model as the base\\nto generate the final implant. The comparative studies demonstrated that the\\nEasyCrania can improve the efficiency of cranial implant design significantly.\\nAnd, the intra- and inter-rater reliability of the software were stable, which\\nwere 87.07+/-1.6% and 87.73+/-1.4% respectively.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2009.13704v1', 'Published': datetime.date(2020, 9, 29), 'Title': 'Cranial Implant Design via Virtual Craniectomy with Shape Priors', 'Authors': 'Franco Matzkin, Virginia Newcombe, Ben Glocker, Enzo Ferrante'}, page_content='Cranial implant design is a challenging task, whose accuracy is crucial in\\nthe context of cranioplasty procedures. This task is usually performed manually\\nby experts using computer-assisted design software. In this work, we propose\\nand evaluate alternative automatic deep learning models for cranial implant\\nreconstruction from CT images. The models are trained and evaluated using the\\ndatabase released by the AutoImplant challenge, and compared to a baseline\\nimplemented by the organizers. We employ a simulated virtual craniectomy to\\ntrain our models using complete skulls, and compare two different approaches\\ntrained with this procedure. The first one is a direct estimation method based\\non the UNet architecture. The second method incorporates shape priors to\\nincrease the robustness when dealing with out-of-distribution implant shapes.\\nOur direct estimation method outperforms the baselines provided by the\\norganizers, while the model with shape priors shows superior performance when\\ndealing with out-of-distribution cases. Overall, our methods show promising\\nresults in the difficult task of cranial implant design.')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_retriever = ArxivRetriever(load_max_docs=3, get_ful_documents=True)\n",
    "arxiv_retriever.invoke('What is the order of the cranial nerves?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "  question: str\n",
    "\n",
    "  specialized_srcs: List[str]\n",
    "\n",
    "  step_back_query: str\n",
    "  rewritten_query: str\n",
    "  subqueries: List[str]\n",
    "\n",
    "  generated_docs: List[str]\n",
    "\n",
    "  documents: Annotated[list, operator.add]\n",
    "\n",
    "  web_search: str\n",
    "\n",
    "  generation: str\n",
    "  generations_num: int\n",
    "\n",
    "def determine_specialized_srcs(state):\n",
    "  print('---DETERMINE SPECIALIZED SOURCES---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  res = question_router.invoke({'question': question})\n",
    "  srcs = [src.strip().lower() for src in res.sources]\n",
    "  \n",
    "  return {'specialized_srcs': srcs}\n",
    "\n",
    "def route_question(state):\n",
    "  print('---ROUTE QUESTION---')\n",
    "\n",
    "  sources = state['specialized_srcs']\n",
    "\n",
    "  if len(sources) == 0:\n",
    "    print('---ROUTE QUESTION TO WEB SEARCH---')\n",
    "    return 'websearch'\n",
    "  else:\n",
    "    print(f'---ROUTE QUESTION TO SPECIALIZED SOURCES: {\", \".join([source.upper() for source in sources])}---')\n",
    "    return 'specialized_srcs'\n",
    "\n",
    "def generate_step_back_query(state):\n",
    "  print('---GENERATE STEP-BACK QUERY---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  step_back_query = step_back_chain.invoke({'question': question})\n",
    "\n",
    "  return {'step_back_query': step_back_query}\n",
    "\n",
    "def generate_rewritten_query(state):\n",
    "  print('---GENERATE REWRITTEN QUERY---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  rewritten_query = rewrite_query_chain.invoke({'question': question})\n",
    "\n",
    "  return {'rewritten_query': rewritten_query}\n",
    "\n",
    "def generate_subqueries(state):\n",
    "  print('---GENERATE SUBQUERIES---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  try:\n",
    "    decomposition_answer = decomposition_chain.invoke({'question': question})\n",
    "    subqueries = decomposition_answer.subqueries\n",
    "    # Limit to a maximum of four subqueries\n",
    "    subqueries = subqueries[:4]\n",
    "  except:\n",
    "    subqueries = []\n",
    "\n",
    "  print(f'---FINAL SUBQUERIES NUMBER: {len(subqueries)}---')\n",
    "\n",
    "  return {'subqueries': subqueries}\n",
    "\n",
    "def generate_hyde_docs(state):\n",
    "  print('---GENERATE HYDE DOCUMENTS---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  generated_docs = []\n",
    "\n",
    "  for query in queries:\n",
    "    generated_doc = hyde_chain.invoke({'question': query})\n",
    "    generated_docs.append(generated_doc)\n",
    "\n",
    "  return {'question': question, 'generated_docs': generated_docs}\n",
    "\n",
    "def vector_store_retriever_node(state):\n",
    "  generated_docs = state['generated_docs']\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'vectorstore' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM VECTOR STORE---')\n",
    "\n",
    "  documents = []\n",
    "\n",
    "  for generated_doc in generated_docs:\n",
    "    documents.extend(retriever.invoke(generated_doc))\n",
    "\n",
    "  unique_documents = []\n",
    "  seen_contents = set()\n",
    "\n",
    "  for document in documents:\n",
    "    if document.page_content in seen_contents:\n",
    "      continue\n",
    "\n",
    "    unique_documents.append(document)\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "  print('vector store documents len', len(unique_documents))\n",
    "\n",
    "  return {'documents': unique_documents}\n",
    "\n",
    "def pub_med_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'pubmed' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM PUBMED---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(pub_med_retriever.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  unique_documents = []\n",
    "  seen_contents = set()\n",
    "\n",
    "  for document in documents:\n",
    "    if document.page_content in seen_contents:\n",
    "      continue\n",
    "\n",
    "    unique_documents.append(document)\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "  print('pubmed documents len', len(unique_documents))\n",
    "\n",
    "  return {'documents': unique_documents}\n",
    "\n",
    "def arxiv_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'arxiv' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM ARXIV---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(arxiv_retriever.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  unique_documents = []\n",
    "  seen_contents = set()\n",
    "\n",
    "  for document in documents:\n",
    "    if document.page_content in seen_contents:\n",
    "      continue\n",
    "\n",
    "    unique_documents.append(document)\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "  print('pubmed documents len', len(unique_documents))\n",
    "\n",
    "  return {'documents': unique_documents}\n",
    "\n",
    "def grade_documents(state):\n",
    "  print('---CHECK DOCUMENT RELEVANCE TO QUESTION---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "\n",
    "  print('final documents len', len(documents))\n",
    "\n",
    "  # Score each doc\n",
    "  filtered_docs = []\n",
    "  web_search = 'No'\n",
    "  for index, d in enumerate(documents):\n",
    "    print(f'---GRADE DOCUMENT ({index + 1}/{len(documents)})---')\n",
    "    try:\n",
    "      score = docs_grade_grader.invoke({'question': question, 'document': d.page_content})\n",
    "      grade = score.binary_score\n",
    "    except:\n",
    "      grade = 'No'\n",
    "    # Document relevant\n",
    "    if grade.lower() == 'yes':\n",
    "      print('---GRADE: DOCUMENT RELEVANT---')\n",
    "      filtered_docs.append(d)\n",
    "    # Document not relevant\n",
    "    else:\n",
    "      print('---GRADE: DOCUMENT NOT RELEVANT---')\n",
    "      # We do not include the document in filtered_docs\n",
    "      # We set a flag to indicate that we want to run web search\n",
    "      web_search = 'Yes'\n",
    "      continue\n",
    "\n",
    "  print(f'---FINAL DOCUMENTS NUMBER: {len(filtered_docs)}---')\n",
    "\n",
    "  return {\n",
    "    'question': question,\n",
    "    'documents': filtered_docs,\n",
    "    'web_search': web_search,\n",
    "  }\n",
    "\n",
    "def decide_to_generate(state):\n",
    "  print('---ASSESS GRADED DOCUMENTS---')\n",
    "\n",
    "  web_search = state['web_search']\n",
    "\n",
    "  if web_search == 'Yes':\n",
    "    # Some documents have been filtered check_relevance\n",
    "    # We will re-generate a new query\n",
    "    print('---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---')\n",
    "    return 'websearch'\n",
    "  else:\n",
    "    # We have relevant documents, so generate answer\n",
    "    print('---DECISION: GENERATE---')\n",
    "    return 'generate'\n",
    "\n",
    "def web_search(state):\n",
    "  print('---WEB SEARCH---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state.get('documents')\n",
    "\n",
    "  docs = web_search_tool.invoke({'query': question})\n",
    "  web_results = '\\n'.join([d['content'] for d in docs])\n",
    "  web_results = Document(page_content=web_results)\n",
    "  if documents is not None:\n",
    "    documents.append(web_results)\n",
    "  else:\n",
    "    documents = [web_results]\n",
    "\n",
    "  return {\n",
    "    'question': question,\n",
    "    'documents': documents,\n",
    "  }\n",
    "\n",
    "def generate(state):\n",
    "  print('---GENERATE---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "  generations_num = state.get('generations_num', 0)\n",
    "\n",
    "  # RAG generation\n",
    "  generation = rag_chain.invoke({'context': documents, 'question': question})\n",
    "  return {\n",
    "    'question': question,\n",
    "    'documents': documents,\n",
    "    'generation': generation,\n",
    "    'generations_num': generations_num + 1,\n",
    "  }\n",
    "\n",
    "def grade_generation(state):\n",
    "  print('---CHECK HALLUCINATIONS---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "  generation = state['generation']\n",
    "  generations_num = state['generations_num']\n",
    "\n",
    "  if generations_num >= 2:\n",
    "    return 'useful'\n",
    "\n",
    "  try:\n",
    "    score = hallucination_grader.invoke({'documents': documents, 'generation': generation})\n",
    "    grade = score.binary_score\n",
    "  except:\n",
    "    grade = 'no'\n",
    "\n",
    "  # Check hallucination\n",
    "  if grade == 'yes':\n",
    "    print('---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---')\n",
    "    # Check question-answering\n",
    "    print('---GRADE GENERATION vs QUESTION---')\n",
    "    try:\n",
    "      score = answer_grader.invoke({'question': question,'generation': generation})\n",
    "      grade = score.binary_score\n",
    "    except:\n",
    "      grade = 'no'\n",
    "\n",
    "    if grade == 'yes':\n",
    "      print('---DECISION: GENERATION ADDRESSES QUESTION---')\n",
    "      return 'useful'\n",
    "    else:\n",
    "      print('---DECISION: GENERATION DOES NOT ADDRESS QUESTION---')\n",
    "      return 'not useful'\n",
    "  else:\n",
    "    print('---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---')\n",
    "    return 'not supported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7916d05cec70>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node('determine_specialized_srcs', determine_specialized_srcs)\n",
    "\n",
    "workflow.add_node('generate_step_back_query', generate_step_back_query)\n",
    "workflow.add_node('generate_rewritten_query', generate_rewritten_query)\n",
    "workflow.add_node('generate_subqueries', generate_subqueries)\n",
    "\n",
    "workflow.add_node('generate_hyde_docs', generate_hyde_docs)\n",
    "\n",
    "workflow.add_node('vector_store_retriever', vector_store_retriever_node)\n",
    "workflow.add_node('pub_med_retriever', pub_med_retriever_node)\n",
    "workflow.add_node('arxiv_retriever', arxiv_retriever_node)\n",
    "\n",
    "workflow.add_node('websearch', web_search)\n",
    "workflow.add_node('generate', generate)\n",
    "workflow.add_node('grade_documents', grade_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7916d05cec70>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(START, 'determine_specialized_srcs')\n",
    "workflow.add_conditional_edges(\n",
    "  'determine_specialized_srcs',\n",
    "  route_question,\n",
    "  {\n",
    "    'websearch': 'websearch',\n",
    "    'specialized_srcs': 'generate_step_back_query',\n",
    "  },\n",
    ")\n",
    "\n",
    "workflow.add_edge('generate_step_back_query', 'generate_rewritten_query')\n",
    "workflow.add_edge('generate_rewritten_query', 'generate_subqueries')\n",
    "workflow.add_edge('generate_subqueries', 'generate_hyde_docs')\n",
    "\n",
    "workflow.add_edge('generate_hyde_docs', 'vector_store_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'pub_med_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'arxiv_retriever')\n",
    "\n",
    "workflow.add_edge('vector_store_retriever', 'grade_documents')\n",
    "workflow.add_edge('pub_med_retriever', 'grade_documents')\n",
    "workflow.add_edge('arxiv_retriever', 'grade_documents')\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "  'grade_documents',\n",
    "  decide_to_generate,\n",
    "  {\n",
    "    'websearch': 'websearch',\n",
    "    'generate': 'generate',\n",
    "  },\n",
    ")\n",
    "workflow.add_edge('websearch', 'generate')\n",
    "workflow.add_conditional_edges(\n",
    "  'generate',\n",
    "  grade_generation,\n",
    "  {\n",
    "    'not supported': 'generate',\n",
    "    'useful': END,\n",
    "    'not useful': 'websearch',\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.invoke({\"question\": 'What is the order of the cranial nerves?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>Where are alpha-4 beta-2 acetylcholine recepto...</td>\n",
       "      <td>Brain\\nPre and post synaptically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>In the spinal cord where are you most likely t...</td>\n",
       "      <td>Ventral horn which contains the motor neurons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>What is apraxia ?</td>\n",
       "      <td>Selective inability to perform complex motor acts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Explain zone 3 of the flow zones in an upright...</td>\n",
       "      <td>Flow determined by arteriovenous pressure diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Which receptors are activated by menthol and c...</td>\n",
       "      <td>Menthol = trpm8\\nCapsaicin= trpv1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>What are the subdivisions of the vertebral col...</td>\n",
       "      <td>Cervical = 8\\nThoracic= 12\\nLumbar=5 \\nSacral=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>How many neurons are present in human cerebral...</td>\n",
       "      <td>10 to the power of 10 neurons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Do patients normally die form PD and what are ...</td>\n",
       "      <td>No they normally die of a complication of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>What does the sodium-potassium ATPase pump do ?</td>\n",
       "      <td>It maintains the gradient across the membrane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Which tract originates in the red nucleus and ...</td>\n",
       "      <td>The rubrospinal tract which is important in fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1052 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "1013  Where are alpha-4 beta-2 acetylcholine recepto...   \n",
       "300   In the spinal cord where are you most likely t...   \n",
       "967                                   What is apraxia ?   \n",
       "408   Explain zone 3 of the flow zones in an upright...   \n",
       "428   Which receptors are activated by menthol and c...   \n",
       "...                                                 ...   \n",
       "29    What are the subdivisions of the vertebral col...   \n",
       "103   How many neurons are present in human cerebral...   \n",
       "521   Do patients normally die form PD and what are ...   \n",
       "186     What does the sodium-potassium ATPase pump do ?   \n",
       "144   Which tract originates in the red nucleus and ...   \n",
       "\n",
       "                                                 answer  \n",
       "1013                   Brain\\nPre and post synaptically  \n",
       "300       Ventral horn which contains the motor neurons  \n",
       "967   Selective inability to perform complex motor acts  \n",
       "408   Flow determined by arteriovenous pressure diff...  \n",
       "428                   Menthol = trpm8\\nCapsaicin= trpv1  \n",
       "...                                                 ...  \n",
       "29    Cervical = 8\\nThoracic= 12\\nLumbar=5 \\nSacral=...  \n",
       "103                       10 to the power of 10 neurons  \n",
       "521   No they normally die of a complication of the ...  \n",
       "186   It maintains the gradient across the membrane ...  \n",
       "144   The rubrospinal tract which is important in fi...  \n",
       "\n",
       "[1052 rows x 2 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.read_csv('brainscape.csv')\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cached RAGs responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_path = Path('cache.json')\n",
    "\n",
    "if not os.path.exists(cache_path):\n",
    "  data = {}\n",
    "  with open(cache_path, 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "with open(cache_path, 'r') as f:\n",
    "  cache = json.load(f)\n",
    "\n",
    "len(cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DETERMINE SPECIALIZED SOURCES---\n",
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO SPECIALIZED SOURCES: VECTORSTORE---\n",
      "---GENERATE STEP-BACK QUERY---\n",
      "---GENERATE REWRITTEN QUERY---\n",
      "---GENERATE SUBQUERIES---\n",
      "---FINAL SUBQUERIES NUMBER: 4---\n",
      "---GENERATE HYDE DOCUMENTS---\n"
     ]
    }
   ],
   "source": [
    "questions = list(qa_df['question'].tolist())\n",
    "expected_answers = list(qa_df['answer'].tolist())\n",
    "predicted_answers = []\n",
    "\n",
    "for index, question in tqdm(enumerate(questions)):\n",
    "  if not question in cache:\n",
    "    cache[question] = app.invoke({'question': question})['generation']\n",
    "\n",
    "  predicted_answers.append(cache[question])\n",
    "\n",
    "  with open(cache_path, 'w') as f:\n",
    "    json.dump(cache, f)\n",
    "\n",
    "cos_score = embeddings_cosine_sim_metric(expected_answers, predicted_answers)\n",
    "bleu_score = bleu_metric(expected_answers, predicted_answers)\n",
    "rogue_1_score = rogue_1_metric(expected_answers, predicted_answers)\n",
    "rogue_l_score = rogue_l_metric(expected_answers, predicted_answers)\n",
    "\n",
    "cos_score, bleu_score, rogue_1_score, rogue_l_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
