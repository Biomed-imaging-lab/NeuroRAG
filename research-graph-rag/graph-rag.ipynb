{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: unidecode in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (1.3.8)\n",
      "Requirement already satisfied: scikit-learn in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (1.5.2)\n",
      "Requirement already satisfied: tqdm in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (4.66.5)\n",
      "Requirement already satisfied: llm-blender in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: rouge-score in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: xmltodict in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.14.2)\n",
      "Requirement already satisfied: arxiv in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (2.1.3)\n",
      "Requirement already satisfied: biopython in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (1.84)\n",
      "Requirement already satisfied: click in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: transformers in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (4.45.2)\n",
      "Requirement already satisfied: torch in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (2.5.0)\n",
      "Requirement already satisfied: accelerate in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (1.0.1)\n",
      "Requirement already satisfied: safetensors in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (0.4.5)\n",
      "Requirement already satisfied: dataclasses-json in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (0.6.7)\n",
      "Requirement already satisfied: sentencepiece in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (0.2.0)\n",
      "Requirement already satisfied: protobuf in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from llm-blender) (4.25.5)\n",
      "Requirement already satisfied: absl-py in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests~=2.32.0->arxiv) (2024.8.30)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (23.2)\n",
      "Requirement already satisfied: psutil in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from accelerate->llm-blender) (0.26.1)\n",
      "Requirement already satisfied: filelock in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from torch->llm-blender) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from sympy==1.13.1->torch->llm-blender) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json->llm-blender) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json->llm-blender) (0.9.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from transformers->llm-blender) (0.20.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->llm-blender) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from jinja2->torch->llm-blender) (3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.4)\n",
      "Requirement already satisfied: langchain-core in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.13)\n",
      "Requirement already satisfied: langchain-community in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.3)\n",
      "Requirement already satisfied: langchain_experimental in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-openai in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.2.4)\n",
      "Requirement already satisfied: langchain-chroma in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.1.4)\n",
      "Requirement already satisfied: langchain_mistralai in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: langgraph in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.2.39)\n",
      "Requirement already satisfied: langchainhub in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (0.1.21)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (0.1.137)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-core) (4.12.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-community) (2.6.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-openai) (1.52.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-chroma) (0.5.15)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain-chroma) (0.115.3)\n",
      "Requirement already satisfied: httpx<1,>=0.25.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain_mistralai) (0.27.2)\n",
      "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain_mistralai) (0.4.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchain_mistralai) (0.20.1)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langgraph) (2.0.1)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.32 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langgraph) (0.1.33)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langchainhub) (2.32.0.20241016)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.7.6)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.32.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.7.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.19.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.66.5)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.66.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (31.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.10.10)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.9.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from fastapi<1,>=0.95.2->langchain-chroma) (0.41.0)\n",
      "Requirement already satisfied: anyio in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.0.6)\n",
      "Requirement already satisfied: idna in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpx<1,>=0.25.2->langchain_mistralai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain_mistralai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph) (1.1.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from tokenizers<1,>=0.15.1->langchain_mistralai) (0.26.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from anyio->httpx<1,>=0.25.2->langchain_mistralai) (1.2.2)\n",
      "Requirement already satisfied: pyproject_hooks in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.4.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.2)\n",
      "Requirement already satisfied: filelock in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain_mistralai) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.9.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.35.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.25.5)\n",
      "Requirement already satisfied: sympy in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.48b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (75.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.5.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.4)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (13.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from importlib-resources->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (3.20.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (4.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.1.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/super-pc2/miniconda3/envs/biorag/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk numpy pandas unidecode scikit-learn tqdm llm-blender rouge-score xmltodict arxiv biopython\n",
    "! pip install langchain langchain-core langchain-community langchain_experimental langchain-openai langchain-chroma langchain_mistralai langgraph langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import llm_blender\n",
    "from operator import itemgetter\n",
    "import operator\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from typing import List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from Bio import Entrez, SeqIO\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings.cache import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain_community.llms import Ollama\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import RetryOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.retrievers import PubMedRetriever, ArxivRetriever\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment variables\n",
    "\n",
    "You have to define the following environment variables in the `.env` file, terminal environment, or input field within this Jupyter notebook:\n",
    "1. MISTRAL_API_KEY\n",
    "2. OPENAI_API_KEY\n",
    "3. OPENAI_PROXY\n",
    "4. TAVILY_API_KEY\n",
    "5. ENTREZ_EMAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_variables = [\n",
    "  'MISTRAL_API_KEY',\n",
    "  'OPENAI_API_KEY',\n",
    "  'OPENAI_PROXY',\n",
    "  'TAVILY_API_KEY',\n",
    "  'ENTREZ_EMAIL',\n",
    "]\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "for key in env_variables:\n",
    "  value = os.getenv(key)\n",
    "\n",
    "  if value is None:\n",
    "    value = getpass(key)\n",
    "\n",
    "  os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download NLTK dictionaries\n",
    "\n",
    "These dictionaries are needed for further text preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ids = [\n",
    "  'punkt_tab',\n",
    "  'punkt',\n",
    "  'stopwords',\n",
    "  'wordnet',\n",
    "]\n",
    "\n",
    "for dict_id in dict_ids:\n",
    "  nltk.download(dict_id, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Define a function for text preprocessing, which is an important step before calculating any metrics. This preprocessing function will help in cleaning the text data, making it ready for further analysis. The preprocessing involves several steps:\n",
    "1. Lowercasing\n",
    "2. Stopwords removal\n",
    "3. Lemmatization\n",
    "4. Remove accents from characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def preprocess(corpus: str) -> str:\n",
    "  corpus = corpus.lower()\n",
    "  stopset = nltk.corpus.stopwords.words('english') + nltk.corpus.stopwords.words('russian') + list(string.punctuation)\n",
    "  tokens = nltk.word_tokenize(corpus)\n",
    "  tokens = [t for t in tokens if t not in stopset]\n",
    "  tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "  corpus = ' '.join(tokens)\n",
    "  corpus = unidecode(corpus)\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Initialization\n",
    "\n",
    "Here we are initializing the Llama 3 embeddings model. The `OllamaEmbeddings` class is a component of the Ollama library, a set of pre-trained language models. This model is capable of embedding corpora of any length into a 4096-dimensional vector.\n",
    "\n",
    "The use of `OllamaEmbeddings` requires the installation of a local Ollama server, which can be found at https://ollama.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model='llama3.1')\n",
    "store = LocalFileStore(\"./.embeddings_cache\")\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "  embeddings,\n",
    "  store,\n",
    "  namespace=embeddings.model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average embeddings cosine similarity metric\n",
    "\n",
    "This function calculates the average cosine similarity between expected answers and LLM predicted answers using their respective embeddings. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them:\n",
    "\n",
    "$$\n",
    "K(a, b) = \\frac{\\sum \\limits_{i=1}^n a_i b_i}{\\sqrt{\\sum \\limits_{i=1}^n a_i^2} \\cdot \\sqrt{\\sum \\limits_{i=1}^n b_i^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_cosine_sim_metric(expected_answers: list[str], predicted_answers: list[str]) -> float:\n",
    "  results = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    expected_embedding = np.array(cached_embeddings.embed_query(expected_answer))\n",
    "    predicted_embedding = np.array(cached_embeddings.embed_query(predicted_answer))\n",
    "\n",
    "    sim = cosine_similarity(\n",
    "      expected_embedding.reshape(1, -1),\n",
    "      predicted_embedding.reshape(1, -1),\n",
    "    )[0][0]\n",
    "\n",
    "    results.append(sim)\n",
    "\n",
    "  return np.mean(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothie_f = nltk.translate.bleu_score.SmoothingFunction().method4\n",
    "\n",
    "def bleu_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    predicted_tokens = nltk.word_tokenize(predicted_answer)\n",
    "    expected_tokens = [nltk.word_tokenize(expected_answer)]\n",
    "\n",
    "    score = nltk.translate.bleu_score.sentence_bleu(\n",
    "      expected_tokens,\n",
    "      predicted_tokens,\n",
    "      smoothing_function=smoothie_f,\n",
    "    )\n",
    "\n",
    "    scores.append(score)\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_1_scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "def rogue_1_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_1_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rouge1'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "rogue_l_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def rogue_l_metric(expected_answers, predicted_answers):\n",
    "  scores = []\n",
    "\n",
    "  for expected_answer, predicted_answer in zip(expected_answers, predicted_answers):\n",
    "    expected_answer = preprocess(expected_answer)\n",
    "    predicted_answer = preprocess(predicted_answer)\n",
    "\n",
    "    result = rogue_l_scorer.score(expected_answer, predicted_answer)\n",
    "\n",
    "    scores.append(result['rougeL'])\n",
    "\n",
    "  return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dir = Path('./docs')\n",
    "docs_cache_dir = Path('./.docs_cache')\n",
    "raw_docs_pkl_path = docs_cache_dir / 'parsed_docs_cache.pkl'\n",
    "\n",
    "docs = None\n",
    "\n",
    "if os.path.exists(raw_docs_pkl_path):\n",
    "  with open(raw_docs_pkl_path, 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "else:\n",
    "  docs = []\n",
    "  for file in docs_dir.iterdir():\n",
    "    docs.extend(PDFMinerLoader(file).load())\n",
    "\n",
    "  with open(raw_docs_pkl_path, 'wb') as f:\n",
    "    pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17443"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_docs_pkl_path = docs_cache_dir / 'splitted_docs_cache.pkl'\n",
    "\n",
    "if os.path.exists(splitted_docs_pkl_path):\n",
    "  with open(splitted_docs_pkl_path, 'rb') as f:\n",
    "    splitted_docs = pickle.load(f)\n",
    "else:\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=750,\n",
    "    chunk_overlap=250,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "  )\n",
    "  splitted_docs = text_splitter.create_documents([doc.page_content for doc in docs])\n",
    "\n",
    "  with open(splitted_docs_pkl_path, 'wb') as f:\n",
    "    pickle.dump(splitted_docs, f)\n",
    "\n",
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "  embedding_function=cached_embeddings,\n",
    "  persist_directory='./chroma_db'\n",
    ")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define JSON extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(response):\n",
    "  json_pattern = r'\\{.*?\\}'\n",
    "  match = re.search(json_pattern, response, re.DOTALL)\n",
    "\n",
    "  if match:\n",
    "    return match.group().strip().replace('\\\\\\\\', '\\\\')\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='llama3.1', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sources=[]\n",
      "sources=['vectorstore', 'ncbi_protein']\n"
     ]
    }
   ],
   "source": [
    "class RouteQuery(BaseModel):\n",
    "  sources: List[str] = Field(\n",
    "    description='Given a user question select the retrieval methods you consider the most appropriate for addressing this question. You may also return an empty array if no methods are required.',\n",
    "  )\n",
    "\n",
    "route_parser = PydanticOutputParser(pydantic_object=RouteQuery)\n",
    "route_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=route_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "route_template = \"\"\"\n",
    "You are an expert at selecting retrieval methods.\n",
    "Given a user question select the retrieval methods you consider the most appropriate for addressing user question.\n",
    "You may also return an empty array if no methods are required.\n",
    "\n",
    "Possible retrieval methods:\n",
    "1. The \"vectorstore\" retriever contains documents related to neurobiology and medicine. Use the vectorstore for questions on these topics.\n",
    "2. The \"pubmed\" retriever contains biomedical literature and research articles. It is particularly useful for answering detailed questions about medical research, clinical studies, and scientific discoveries.\n",
    "3. The \"arxiv\" retriever contains preprints of research papers across various scientific fields, including physics, mathematics, computer science, and biology. Use the arxiv for questions on recent scientific research and theoretical studies in these areas.\n",
    "4. The \"ncbi_protein\" retriever contains protein sequence and functional information. Use the NCBI protein DB for questions related to protein sequences, structures, and functions.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\"\"\"\n",
    "route_prompt = PromptTemplate(\n",
    "  template=route_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': route_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "question_router = RunnableParallel(\n",
    "  completion=route_prompt | llm | extract_json, prompt_value=route_prompt\n",
    ") | RunnableLambda(lambda x: route_retry_parser.parse_with_prompt(**x))\n",
    "print(question_router.invoke({'question': 'Who will the Bears draft first in the NFL draft?'}))\n",
    "print(question_router.invoke({'question': 'What are the functions of the oculomotor nerve?'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade documents chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeDocuments(binary_score='yes')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "  binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "docs_grade_parser = PydanticOutputParser(pydantic_object=GradeDocuments)\n",
    "docs_grade_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=docs_grade_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "docs_grade_template = \"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a user question.\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Retrieved document:\n",
    "{document}\n",
    "\"\"\"\n",
    "docs_grade_prompt = PromptTemplate(\n",
    "  template=docs_grade_template,\n",
    "  input_variables=['document', 'question'],\n",
    "  partial_variables={'format_instructions': docs_grade_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "docs_grade_grader = RunnableParallel(\n",
    "  completion=docs_grade_prompt | llm | extract_json, prompt_value=docs_grade_prompt\n",
    ") | RunnableLambda(lambda x: docs_grade_retry_parser.parse_with_prompt(**x))\n",
    "docs_grade_grader.invoke({'question': 'What is the color of the sky?', 'document': 'The color of the sky is blue'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucinations chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "  binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "hallucination_parser = PydanticOutputParser(pydantic_object=GradeHallucinations)\n",
    "hallucination_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=hallucination_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "hallucination_template = \"\"\"\n",
    "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Set of facts:\n",
    "{documents}\n",
    "\n",
    "LLM generation:\n",
    "{generation}\n",
    "\"\"\"\n",
    "hallucination_prompt = PromptTemplate(\n",
    "  template=hallucination_template,\n",
    "  input_variables=['question', 'generation'],\n",
    "  partial_variables={'format_instructions': hallucination_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "hallucination_grader = RunnableParallel(\n",
    "  completion=hallucination_prompt | llm | extract_json, prompt_value=hallucination_prompt\n",
    ") | RunnableLambda(lambda x: hallucination_retry_parser.parse_with_prompt(**x))\n",
    "print(hallucination_grader.invoke({'documents': ['Sky is blue'], 'generation': 'The color of the sky is blue'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer grade chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "  binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "grade_parser = PydanticOutputParser(pydantic_object=GradeAnswer)\n",
    "grade_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=grade_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "grade_template = \"\"\"\n",
    "You are a grader assessing whether an answer addresses / resolves a question. \\n\n",
    "Give a binary score 'yes' or 'no'. 'yes' means that the answer resolves the question.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "LLM generation:\n",
    "{generation}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "grade_prompt = PromptTemplate(\n",
    "  template=grade_template,\n",
    "  input_variables=['question', 'generation'],\n",
    "  partial_variables={'format_instructions': grade_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "answer_grader = RunnableParallel(\n",
    "  completion=grade_prompt | llm | extract_json, prompt_value=grade_prompt\n",
    ") | RunnableLambda(lambda x: grade_retry_parser.parse_with_prompt(**x))\n",
    "print(answer_grader.invoke({\"question\": \"What is the order of the cranial nerves?\", 'generation': 'I do not know.'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyDE chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a scientific paper-style passage answering the question:\\n\\n**Title:** The Cranial Nerve Order: A Review and Classification\\n\\n**Abstract:**\\nThe cranial nerves are a complex group of 12 pairs of nerves that arise directly from the brain, playing crucial roles in various physiological functions. Despite their importance, the order of these nerves has been a subject of interest among neuroscientists for centuries. This review aims to provide an overview of the classification and nomenclature of the cranial nerves, highlighting their distinct characteristics and functional specializations.\\n\\n**Introduction:**\\nThe cranial nerves are a unique group of nerves that emerge from the brain, serving as the primary motor and sensory pathways between the central nervous system and various peripheral structures. The order of these nerves has been traditionally classified based on their anatomical origin, with the first nerve emerging from the forebrain and the subsequent nerves arising from the midbrain, pons, and medulla oblongata.\\n\\n**Results:**\\nThe cranial nerves are listed in the following order:\\n\\n1. **Olfactory Nerve (I)**: The first cranial nerve to emerge, responsible for transmitting sensory information related to smell.\\n2. **Optic Nerve (II)**: The second cranial nerve, involved in visual perception and transmission of light signals from the retina to the brain.\\n3. **Occulomotor Nerve (III)**: The third cranial nerve, controlling eye movement, accommodation, and pupillary constriction.\\n4. **Trochlear Nerve (IV)**: The fourth cranial nerve, responsible for innervating the superior oblique muscle of the eye.\\n5. **Trigeminal Nerve (V)**: A complex nerve with three branches (ophthalmic, maxillary, and mandibular), involved in facial sensation, motor control of mastication, and other functions.\\n6. **Abducens Nerve (VI)**: The sixth cranial nerve, responsible for controlling eye movement and innervating the lateral rectus muscle.\\n7. **Facial Nerve (VII)**: A multifunctional nerve involved in facial expression, taste sensation, and motor control of the stapedius muscle.\\n8. **Vestibulocochlear Nerve (VIII)**: The eighth cranial nerve, responsible for transmitting auditory and vestibular information from the inner ear to the brain.\\n9. **Glossopharyngeal Nerve (IX)**: A complex nerve involved in swallowing, taste sensation, and motor control of various muscles.\\n10. **Vagus Nerve (X)**: The tenth cranial nerve, responsible for controlling various visceral functions, including heart rate, digestion, and respiration.\\n11. **Spinal Accessory Nerve (XI)**: A motor nerve involved in the innervation of neck and shoulder muscles.\\n12. **Hypoglossal Nerve (XII)**: The twelfth cranial nerve, responsible for controlling tongue movement and other functions.\\n\\n**Conclusion:**\\nIn conclusion, the order of the cranial nerves is a well-established classification system that reflects their anatomical origin and functional specializations. Understanding this order is essential for neuroscientists, clinicians, and researchers working in various fields related to neuroscience and medicine.\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_template = \"\"\"\n",
    "Please write a scientific paper passage to answer the question\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Passage:\n",
    "\"\"\"\n",
    "hyde_prompt = ChatPromptTemplate.from_template(hyde_template)\n",
    "hyde_chain = hyde_prompt | llm | StrOutputParser()\n",
    "\n",
    "hyde_chain.invoke({\"question\": 'What is the order of the cranial nerves ?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the definition of a neurological syndrome?'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_back_template = \"\"\"\n",
    "You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate a step-back query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Step-back query:\n",
    "\"\"\"\n",
    "step_back_prompt = ChatPromptTemplate.from_template(step_back_template)\n",
    "step_back_chain = step_back_prompt | llm | StrOutputParser()\n",
    "\n",
    "step_back_chain.invoke({\"question\": 'What is Benedict’s syndrome?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s a rewritten version of the query that\\'s more specific and detailed:\\n\\n\"What is the correct anatomical order of the 12 pairs of cranial nerves, including their names (I-XII), functions, and any notable characteristics or associations?\"\\n\\nThis revised query includes more specific details to help guide the retrieval process in a RAG system. By asking for the \"correct anatomical order\", I\\'ve added a level of precision that can help filter out irrelevant information. Additionally, by specifying the inclusion of names, functions, and notable characteristics, I\\'ve provided more context for the model to retrieve relevant information.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewrite_query_template = \"\"\"\n",
    "You are an AI assistant tasked with reformulating user queries to improve retrieval in a RAG system.\n",
    "Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Rewritten query:\n",
    "\"\"\"\n",
    "rewrite_query_prompt = ChatPromptTemplate.from_template(rewrite_query_template)\n",
    "rewrite_query_chain = rewrite_query_prompt | llm | StrOutputParser()\n",
    "\n",
    "rewrite_query_chain.invoke({\"question\": 'What is the order of the cranial nerves?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subqueries=[\"What are the symptoms of Benedict's syndrome?\", \"What are the causes of Benedict's syndrome?\", \"How is Benedict's syndrome diagnosed?\", \"What are the treatment options for Benedict's syndrome?\"]\n"
     ]
    }
   ],
   "source": [
    "class DecompositionAnswer(BaseModel):\n",
    "  subqueries: List[str] = Field(description=\"Given the original query, decompose it into 2-4 simpler sub-queries as json array of strings\")\n",
    "\n",
    "decomposition_parser = PydanticOutputParser(pydantic_object=DecompositionAnswer)\n",
    "decomposition_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=decomposition_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "decomposition_template = \"\"\"\n",
    "You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "Sub-queries:\n",
    "1. What are the impacts of climate change on biodiversity?\n",
    "2. How does climate change affect the oceans?\n",
    "3. What are the effects of climate change on agriculture?\n",
    "4. What are the impacts of climate change on human health?\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "decomposition_prompt = PromptTemplate(\n",
    "  template=decomposition_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': decomposition_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "decomposition_chain = RunnableParallel(\n",
    "  completion=decomposition_prompt | llm | extract_json, prompt_value=decomposition_prompt\n",
    ") | RunnableLambda(lambda x: decomposition_retry_parser.parse_with_prompt(**x))\n",
    "print(decomposition_chain.invoke({\"question\": \"What is Benedict’s syndrome?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /home/super-pc2/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    }
   ],
   "source": [
    "blender = llm_blender.Blender()\n",
    "blender.loadranker('llm-blender/PairRM', device='cuda')\n",
    "blender.loadfuser('llm-blender/gen_fuser_3b', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "Fusing candidates: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The order of the cranial nerves is as follows: I. Olfactory II. Optic III. Oculomotor IV. Trochlear V. Trigeminal VI. Abducens VII. Facial VIII. Auditory (or vestibulocochlear) IX. Glossopharyngeal X. Vagus XI. Spinal accessory XII. Hypoglossal'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = hub.pull('rlm/rag-prompt')\n",
    "\n",
    "llama_llm = Ollama(model='llama3.1', temperature=0)\n",
    "mistral_llm = ChatMistralAI(model='mistral-large-latest', temperature=0)\n",
    "gpt_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "llama_chain = prompt | llama_llm | StrOutputParser()\n",
    "mistral_chain = prompt | mistral_llm | StrOutputParser()\n",
    "gpt_chain = prompt | gpt_llm | StrOutputParser()\n",
    "\n",
    "def fuse_generations(dict):\n",
    "  question = dict['question']\n",
    "\n",
    "  llama_res = dict['llama_res']\n",
    "  mistral_res = dict['mistral_res']\n",
    "  gpt_res = dict['gpt_res']\n",
    "  answers = [llama_res, mistral_res, gpt_res]\n",
    "\n",
    "  fuse_generations, ranks = blender.rank_and_fuse(\n",
    "    [question],\n",
    "    [answers],\n",
    "    instructions=[''],\n",
    "    return_scores=False,\n",
    "    batch_size=2,\n",
    "    top_k=3\n",
    "  )\n",
    "  return fuse_generations[0]\n",
    "\n",
    "rag_chain = (\n",
    "  {\n",
    "    'llama_res': llama_chain,\n",
    "    'mistral_res': mistral_chain,\n",
    "    'gpt_res': gpt_chain,\n",
    "    'question': itemgetter('question')\n",
    "  }\n",
    "  | RunnableLambda(fuse_generations)\n",
    ")\n",
    "\n",
    "rag_chain.invoke({\"context\": '', \"question\": 'What is the order of the cranial nerves?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_tool = TavilySearchResults(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PubMed Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pub_med_retriever = PubMedRetriever()\n",
    "pub_med_retriever.invoke('What is the order of the cranial nerves?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Entry ID': 'http://arxiv.org/abs/1912.10601v2', 'Published': datetime.date(2021, 3, 13), 'Title': 'Optimized Cranial Bandeau Remodeling', 'Authors': 'James Drake, Marina Drygala, Ricardo Fukasawa, Jochen Koenemann, Andre Linhares, Thomas Looi, John Phillips, David Qian, Nikoo Saber, Justin Toth, Chris Woodbeck, Jessie Yeung'}, page_content=\"Craniosynostosis, a condition affecting 1 in 2000 infants, is caused by\\npremature fusing of cranial vault sutures, and manifests itself in abnormal\\nskull growth patterns. Left untreated, the condition may lead to severe\\ndevelopmental impairment. Standard practice is to apply corrective cranial\\nbandeau remodeling surgery in the first year of the infant's life. The most\\nfrequent type of surgery involves the removal of the so-called fronto-orbital\\nbar from the patient's forehead and the cutting of well-placed incisions to\\nreshape the skull in order to obtain the desired result. In this paper, we\\npropose a precise optimization model for the above cranial bandeau remodeling\\nproblem and its variants. We have developed efficient algorithms that solve\\nbest incision placement, and show hardness for more general cases in the class.\\nTo the best of our knowledge this paper is the first to introduce optimization\\nmodels for craniofacial surgery applications.\"),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/1706.07649v1', 'Published': datetime.date(2017, 6, 23), 'Title': 'Computer-aided implant design for the restoration of cranial defects', 'Authors': 'Xiaojun Chen, Lu Xu, Xing Li, Jan Egger'}, page_content='Patient-specific cranial implants are important and necessary in the surgery\\nof cranial defect restoration. However, traditional methods of manual design of\\ncranial implants are complicated and time-consuming. Our purpose is to develop\\na novel software named EasyCrania to design the cranial implants conveniently\\nand efficiently. The process can be divided into five steps, which are\\nmirroring model, clipping surface, surface fitting, the generation of the\\ninitial implant and the generation of the final implant. The main concept of\\nour method is to use the geometry information of the mirrored model as the base\\nto generate the final implant. The comparative studies demonstrated that the\\nEasyCrania can improve the efficiency of cranial implant design significantly.\\nAnd, the intra- and inter-rater reliability of the software were stable, which\\nwere 87.07+/-1.6% and 87.73+/-1.4% respectively.'),\n",
       " Document(metadata={'Entry ID': 'http://arxiv.org/abs/2009.13704v1', 'Published': datetime.date(2020, 9, 29), 'Title': 'Cranial Implant Design via Virtual Craniectomy with Shape Priors', 'Authors': 'Franco Matzkin, Virginia Newcombe, Ben Glocker, Enzo Ferrante'}, page_content='Cranial implant design is a challenging task, whose accuracy is crucial in\\nthe context of cranioplasty procedures. This task is usually performed manually\\nby experts using computer-assisted design software. In this work, we propose\\nand evaluate alternative automatic deep learning models for cranial implant\\nreconstruction from CT images. The models are trained and evaluated using the\\ndatabase released by the AutoImplant challenge, and compared to a baseline\\nimplemented by the organizers. We employ a simulated virtual craniectomy to\\ntrain our models using complete skulls, and compare two different approaches\\ntrained with this procedure. The first one is a direct estimation method based\\non the UNet architecture. The second method incorporates shape priors to\\nincrease the robustness when dealing with out-of-distribution implant shapes.\\nOur direct estimation method outperforms the baselines provided by the\\norganizers, while the model with shape priors shows superior performance when\\ndealing with out-of-distribution cases. Overall, our methods show promising\\nresults in the difficult task of cranial implant design.')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_retriever = ArxivRetriever(load_max_docs=3, get_ful_documents=True)\n",
    "arxiv_retriever.invoke('What is the order of the cranial nerves?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI Protein DB retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCBIProteinRetriever(BaseRetriever):\n",
    "  k: int\n",
    "\n",
    "  def __init__(self, k: int):\n",
    "    super().__init__(k=k)\n",
    "\n",
    "    self.k = k\n",
    "\n",
    "    entrez_email = os.getenv('ENTREZ_EMAIL')\n",
    "    if entrez_email == None:\n",
    "      raise ValueError('ENTREZ_EMAIL is not defined')\n",
    "    Entrez.email = entrez_email\n",
    "\n",
    "  def _search_protein(self, query):\n",
    "    handle = Entrez.esearch(db='protein', term=query, retmax=self.k)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return record['IdList']\n",
    "\n",
    "  def _fetch_protein(self, protein_id):\n",
    "    handle = Entrez.efetch(db='protein', id=protein_id, rettype='gb', retmode='text')\n",
    "    record = SeqIO.read(handle, 'genbank')\n",
    "    handle.close()\n",
    "    return record\n",
    "\n",
    "  def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "    protein_ids = self._search_protein(query)\n",
    "    docs = []\n",
    "\n",
    "    for protein_id in protein_ids:\n",
    "      protein_record = self._fetch_protein(protein_id)\n",
    "      molecule_type = protein_record.annotations.get(\"molecule_type\", \"N/A\")\n",
    "      organism = protein_record.annotations.get(\"organism\", \"N/A\")\n",
    "      comment = protein_record.annotations.get(\"comment\", \"N/A\")\n",
    "      page_content = (\n",
    "        f'Protein ID: {protein_id}\\n'\n",
    "        f'Type: {molecule_type}\\n'\n",
    "        f'Name: {protein_record.name}\\n'\n",
    "        f'Organism: {organism}\\n'\n",
    "        f'Description: {protein_record.description}\\n'\n",
    "        f'Comment: {comment}\\n'\n",
    "        f'Sequence: {protein_record.seq}'\n",
    "      )\n",
    "      doc = Document(page_content=page_content)\n",
    "      docs.append(doc)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Protein ID: 157955043\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncbi_protein_retriever = NCBIProteinRetriever(k=3)\n",
    "ncbi_protein_retriever.invoke('ABW05875')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCBI Protein DB chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={}, page_content='Protein ID: 157955043\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD')]\n"
     ]
    }
   ],
   "source": [
    "class NCBIProteinDBAnswer(BaseModel):\n",
    "  query: str = Field(description='Given the original query, please find a protein locus for the NCBI protein database.')\n",
    "\n",
    "ncbi_protein_db_parser = PydanticOutputParser(pydantic_object=NCBIProteinDBAnswer)\n",
    "ncbi_protein_db_retry_parser = RetryOutputParser.from_llm(\n",
    "  parser=ncbi_protein_db_parser,\n",
    "  llm=llm,\n",
    "  max_retries=3,\n",
    ")\n",
    "\n",
    "ncbi_protein_db_template = \"\"\"\n",
    "As an expert in bioinformatics and user query optimization for biological databases, your task is to transform user questions into precise and effective queries suitable for the NCBI protein database.\n",
    "Create a query with only locus of a protein for search within the NCBI protein database.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "ncbi_protein_db_prompt = PromptTemplate(\n",
    "  template=ncbi_protein_db_template,\n",
    "  input_variables=['question'],\n",
    "  partial_variables={'format_instructions': ncbi_protein_db_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "query_extractor = lambda res: res.query\n",
    "\n",
    "ncbi_protein_db_chain = RunnableParallel(\n",
    "  completion=ncbi_protein_db_prompt | llm | extract_json, prompt_value=ncbi_protein_db_prompt\n",
    ") | RunnableLambda(lambda x: ncbi_protein_db_retry_parser.parse_with_prompt(**x)) | query_extractor | ncbi_protein_retriever\n",
    "print(ncbi_protein_db_chain.invoke({\"question\": \"Calculate the frequency of each amino acid in the ABW05875 protein sequence\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "  question: str\n",
    "\n",
    "  specialized_srcs: List[str]\n",
    "\n",
    "  step_back_query: str\n",
    "  rewritten_query: str\n",
    "  subqueries: List[str]\n",
    "\n",
    "  generated_docs: List[str]\n",
    "\n",
    "  documents: Annotated[list, operator.add]\n",
    "\n",
    "  web_search: str\n",
    "\n",
    "  generation: str\n",
    "  generations_num: int\n",
    "\n",
    "def determine_specialized_srcs(state):\n",
    "  print('---DETERMINE SPECIALIZED SOURCES---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  try:\n",
    "    res = question_router.invoke({'question': question})\n",
    "    srcs = [src.strip().lower() for src in res.sources]\n",
    "  except:\n",
    "    srcs = []\n",
    "\n",
    "  return {'specialized_srcs': srcs}\n",
    "\n",
    "def route_question(state):\n",
    "  print('---ROUTE QUESTION---')\n",
    "\n",
    "  sources = state['specialized_srcs']\n",
    "\n",
    "  if len(sources) == 0:\n",
    "    print('---ROUTE QUESTION TO WEB SEARCH---')\n",
    "    return 'websearch'\n",
    "  else:\n",
    "    print(f'---ROUTE QUESTION TO SPECIALIZED SOURCES: {\", \".join([source.upper() for source in sources])}---')\n",
    "    return 'specialized_srcs'\n",
    "\n",
    "def generate_step_back_query(state):\n",
    "  print('---GENERATE STEP-BACK QUERY---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  step_back_query = step_back_chain.invoke({'question': question})\n",
    "\n",
    "  return {'step_back_query': step_back_query}\n",
    "\n",
    "def generate_rewritten_query(state):\n",
    "  print('---GENERATE REWRITTEN QUERY---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  rewritten_query = rewrite_query_chain.invoke({'question': question})\n",
    "\n",
    "  return {'rewritten_query': rewritten_query}\n",
    "\n",
    "def generate_subqueries(state):\n",
    "  print('---GENERATE SUBQUERIES---')\n",
    "\n",
    "  question = state['question']\n",
    "\n",
    "  try:\n",
    "    decomposition_answer = decomposition_chain.invoke({'question': question})\n",
    "    subqueries = decomposition_answer.subqueries\n",
    "    # Limit to a maximum of four subqueries\n",
    "    subqueries = subqueries[:4]\n",
    "  except:\n",
    "    subqueries = []\n",
    "\n",
    "  print(f'---FINAL SUBQUERIES NUMBER: {len(subqueries)}---')\n",
    "\n",
    "  return {'subqueries': subqueries}\n",
    "\n",
    "def generate_hyde_docs(state):\n",
    "  print('---GENERATE HYDE DOCUMENTS---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  generated_docs = []\n",
    "\n",
    "  for query in queries:\n",
    "    generated_doc = hyde_chain.invoke({'question': query})\n",
    "    generated_docs.append(generated_doc)\n",
    "\n",
    "  return {'question': question, 'generated_docs': generated_docs}\n",
    "\n",
    "def vector_store_retriever_node(state):\n",
    "  generated_docs = state['generated_docs']\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'vectorstore' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM VECTOR STORE---')\n",
    "\n",
    "  documents = []\n",
    "\n",
    "  for generated_doc in generated_docs:\n",
    "    documents.extend(retriever.invoke(generated_doc))\n",
    "\n",
    "  unique_documents = []\n",
    "  seen_contents = set()\n",
    "\n",
    "  for document in documents:\n",
    "    if document.page_content in seen_contents:\n",
    "      continue\n",
    "\n",
    "    unique_documents.append(document)\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "  return {'documents': unique_documents}\n",
    "\n",
    "def pub_med_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'pubmed' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM PUBMED---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(pub_med_retriever.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  unique_documents = []\n",
    "  seen_contents = set()\n",
    "\n",
    "  for document in documents:\n",
    "    if document.page_content in seen_contents:\n",
    "      continue\n",
    "\n",
    "    unique_documents.append(document)\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "  return {'documents': unique_documents}\n",
    "\n",
    "def arxiv_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'arxiv' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM ARXIV---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(arxiv_retriever.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  unique_documents = []\n",
    "  seen_contents = set()\n",
    "\n",
    "  for document in documents:\n",
    "    if document.page_content in seen_contents:\n",
    "      continue\n",
    "\n",
    "    unique_documents.append(document)\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "  return {'documents': unique_documents}\n",
    "\n",
    "def ncbi_protein_db_retriever_node(state):\n",
    "  specialized_srcs = state['specialized_srcs']\n",
    "\n",
    "  if 'ncbi_protein' not in specialized_srcs:\n",
    "    return {'documents': []}\n",
    "\n",
    "  print('---RETRIEVE FROM NCBI PROTEIN DB---')\n",
    "\n",
    "  question = state['question']\n",
    "  step_back_query = state['step_back_query']\n",
    "  rewritten_query = state['rewritten_query']\n",
    "  subqueries = state['subqueries']\n",
    "\n",
    "  queries = [question, step_back_query, rewritten_query, *subqueries]\n",
    "  documents = []\n",
    "\n",
    "  for query in queries:\n",
    "    try:\n",
    "      documents.extend(ncbi_protein_db_chain.invoke(query))\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  unique_documents = []\n",
    "  seen_contents = set()\n",
    "\n",
    "  for document in documents:\n",
    "    if document.page_content in seen_contents:\n",
    "      continue\n",
    "\n",
    "    unique_documents.append(document)\n",
    "    seen_contents.add(document.page_content)\n",
    "\n",
    "  return {'documents': unique_documents}\n",
    "\n",
    "def grade_documents(state):\n",
    "  print('---CHECK DOCUMENT RELEVANCE TO QUESTION---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "\n",
    "  # Score each doc\n",
    "  filtered_docs = []\n",
    "  seen_contents = set()\n",
    "  web_search = 'No'\n",
    "  for index, doc in enumerate(documents):\n",
    "    print(f'---GRADE DOCUMENT ({index + 1}/{len(documents)})---')\n",
    "\n",
    "    if doc.page_content in seen_contents:\n",
    "      continue\n",
    "    seen_contents.add(doc.page_content)\n",
    "\n",
    "    try:\n",
    "      score = docs_grade_grader.invoke({'question': question, 'document': doc.page_content})\n",
    "      grade = score.binary_score\n",
    "    except:\n",
    "      grade = 'No'\n",
    "\n",
    "    # Document relevant\n",
    "    if grade.lower() == 'yes':\n",
    "      print('---GRADE: DOCUMENT RELEVANT---')\n",
    "      filtered_docs.append(doc)\n",
    "    # Document not relevant\n",
    "    else:\n",
    "      print('---GRADE: DOCUMENT NOT RELEVANT---')\n",
    "      # We do not include the document in filtered_docs\n",
    "      # We set a flag to indicate that we want to run web search\n",
    "      web_search = 'Yes'\n",
    "      continue\n",
    "\n",
    "  print(f'---FINAL DOCUMENTS NUMBER: {len(filtered_docs)}---')\n",
    "\n",
    "  return {\n",
    "    'question': question,\n",
    "    'documents': filtered_docs,\n",
    "    'web_search': web_search,\n",
    "  }\n",
    "\n",
    "def decide_to_generate(state):\n",
    "  print('---ASSESS GRADED DOCUMENTS---')\n",
    "\n",
    "  web_search = state['web_search']\n",
    "\n",
    "  if web_search == 'Yes':\n",
    "    # Some documents have been filtered check_relevance\n",
    "    # We will re-generate a new query\n",
    "    print('---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---')\n",
    "    return 'websearch'\n",
    "  else:\n",
    "    # We have relevant documents, so generate answer\n",
    "    print('---DECISION: GENERATE---')\n",
    "    return 'generate'\n",
    "\n",
    "def web_search(state):\n",
    "  print('---WEB SEARCH---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state.get('documents')\n",
    "\n",
    "  docs = web_search_tool.invoke({'query': question})\n",
    "  web_results = '\\n'.join([d['content'] for d in docs])\n",
    "  web_results = Document(page_content=web_results)\n",
    "  if documents is not None:\n",
    "    documents.append(web_results)\n",
    "  else:\n",
    "    documents = [web_results]\n",
    "\n",
    "  return {\n",
    "    'question': question,\n",
    "    'documents': documents,\n",
    "  }\n",
    "\n",
    "def generate(state):\n",
    "  print('---GENERATE---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "  generations_num = state.get('generations_num', 0)\n",
    "\n",
    "  # RAG generation\n",
    "  generation = rag_chain.invoke({'context': documents, 'question': question})\n",
    "  return {\n",
    "    'question': question,\n",
    "    'documents': documents,\n",
    "    'generation': generation,\n",
    "    'generations_num': generations_num + 1,\n",
    "  }\n",
    "\n",
    "def grade_generation(state):\n",
    "  print('---CHECK HALLUCINATIONS---')\n",
    "\n",
    "  question = state['question']\n",
    "  documents = state['documents']\n",
    "  generation = state['generation']\n",
    "  generations_num = state['generations_num']\n",
    "\n",
    "  if generations_num >= 2:\n",
    "    return 'useful'\n",
    "\n",
    "  try:\n",
    "    score = hallucination_grader.invoke({'documents': '\\n\\n'.join(map(lambda doc: doc.page_content, documents)), 'generation': generation})\n",
    "    grade = score.binary_score\n",
    "  except:\n",
    "    grade = 'no'\n",
    "\n",
    "  # Check hallucination\n",
    "  if grade == 'yes':\n",
    "    print('---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---')\n",
    "    # Check question-answering\n",
    "    print('---GRADE GENERATION vs QUESTION---')\n",
    "    try:\n",
    "      score = answer_grader.invoke({'question': question,'generation': generation})\n",
    "      grade = score.binary_score\n",
    "    except:\n",
    "      grade = 'no'\n",
    "\n",
    "    if grade == 'yes':\n",
    "      print('---DECISION: GENERATION ADDRESSES QUESTION---')\n",
    "      return 'useful'\n",
    "    else:\n",
    "      print('---DECISION: GENERATION DOES NOT ADDRESS QUESTION---')\n",
    "      return 'not useful'\n",
    "  else:\n",
    "    print('---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---')\n",
    "    return 'not supported'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7b24e60eba60>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_node('determine_specialized_srcs', determine_specialized_srcs)\n",
    "\n",
    "workflow.add_node('generate_step_back_query', generate_step_back_query)\n",
    "workflow.add_node('generate_rewritten_query', generate_rewritten_query)\n",
    "workflow.add_node('generate_subqueries', generate_subqueries)\n",
    "\n",
    "workflow.add_node('generate_hyde_docs', generate_hyde_docs)\n",
    "\n",
    "workflow.add_node('vector_store_retriever', vector_store_retriever_node)\n",
    "workflow.add_node('pub_med_retriever', pub_med_retriever_node)\n",
    "workflow.add_node('arxiv_retriever', arxiv_retriever_node)\n",
    "workflow.add_node('ncbi_protein_db_retriever', ncbi_protein_db_retriever_node)\n",
    "\n",
    "workflow.add_node('websearch', web_search)\n",
    "workflow.add_node('generate', generate)\n",
    "workflow.add_node('grade_documents', grade_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x7b24e60eba60>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.add_edge(START, 'determine_specialized_srcs')\n",
    "workflow.add_conditional_edges(\n",
    "  'determine_specialized_srcs',\n",
    "  route_question,\n",
    "  {\n",
    "    'websearch': 'websearch',\n",
    "    'specialized_srcs': 'generate_step_back_query',\n",
    "  },\n",
    ")\n",
    "\n",
    "workflow.add_edge('generate_step_back_query', 'generate_rewritten_query')\n",
    "workflow.add_edge('generate_rewritten_query', 'generate_subqueries')\n",
    "workflow.add_edge('generate_subqueries', 'generate_hyde_docs')\n",
    "\n",
    "workflow.add_edge('generate_hyde_docs', 'vector_store_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'pub_med_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'arxiv_retriever')\n",
    "workflow.add_edge('generate_hyde_docs', 'ncbi_protein_db_retriever')\n",
    "\n",
    "workflow.add_edge('vector_store_retriever', 'grade_documents')\n",
    "workflow.add_edge('pub_med_retriever', 'grade_documents')\n",
    "workflow.add_edge('arxiv_retriever', 'grade_documents')\n",
    "workflow.add_edge('ncbi_protein_db_retriever', 'grade_documents')\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "  'grade_documents',\n",
    "  decide_to_generate,\n",
    "  {\n",
    "    'websearch': 'websearch',\n",
    "    'generate': 'generate',\n",
    "  },\n",
    ")\n",
    "workflow.add_edge('websearch', 'generate')\n",
    "workflow.add_conditional_edges(\n",
    "  'generate',\n",
    "  grade_generation,\n",
    "  {\n",
    "    'not supported': 'generate',\n",
    "    'useful': END,\n",
    "    'not useful': 'websearch',\n",
    "  },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DETERMINE SPECIALIZED SOURCES---\n",
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO SPECIALIZED SOURCES: NCBI_PROTEIN---\n",
      "---GENERATE STEP-BACK QUERY---\n",
      "---GENERATE REWRITTEN QUERY---\n",
      "---GENERATE SUBQUERIES---\n",
      "---FINAL SUBQUERIES NUMBER: 4---\n",
      "---GENERATE HYDE DOCUMENTS---\n",
      "---RETRIEVE FROM NCBI PROTEIN DB---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE DOCUMENT (1/1)---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---FINAL DOCUMENTS NUMBER: 1---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Fusing candidates: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Count each amino acid in the ABW05875 sequence',\n",
       " 'specialized_srcs': ['ncbi_protein'],\n",
       " 'step_back_query': 'To generate a step-back query that improves context retrieval for the original query \"Count each amino acid in the ABW05875 sequence\", I would ask:\\n\\n\"What is the general process of analyzing protein sequences, and what types of information can be extracted from them?\"\\n\\nThis step-back query aims to retrieve relevant background information on protein sequence analysis, including topics such as:\\n\\n* Sequence annotation\\n* Amino acid composition\\n* Protein structure prediction\\n* Bioinformatics tools and resources\\n\\nBy asking this more general question, the RAG system can provide a broader context that may include relevant information for answering the original query, such as how to count amino acids in a protein sequence.',\n",
       " 'rewritten_query': 'Here\\'s a rewritten query that is more specific, detailed, and likely to retrieve relevant information:\\n\\n\"Count and list the frequency of each individual amino acid (alanine, arginine, asparagine, aspartic acid, cysteine, glutamic acid, glutamine, glycine, histidine, isoleucine, leucine, lysine, methionine, phenylalanine, proline, serine, threonine, tryptophan, tyrosine, valine) in the protein sequence ABW05875.\"\\n\\nThis rewritten query:\\n\\n* Specifies that you want to count each individual amino acid\\n* Lists all 20 standard amino acids for clarity\\n* Includes the specific protein sequence ID (ABW05875) to ensure accurate retrieval of relevant information.',\n",
       " 'subqueries': ['Count the frequency of Alanine (A) in the ABW05875 sequence',\n",
       "  'Count the frequency of Cysteine (C) in the ABW05875 sequence',\n",
       "  'Count the frequency of Glycine (G) in the ABW05875 sequence',\n",
       "  'Count the frequency of other amino acids (X) in the ABW05875 sequence'],\n",
       " 'generated_docs': [\"Here is a scientific paper-style passage answering the question:\\n\\n**Title:** Amino Acid Composition Analysis of the ABW05875 Sequence\\n\\n**Abstract:**\\nThe ABW05875 protein sequence, a putative enzyme of unknown function, has been analyzed to determine its amino acid composition. This study aimed to count and quantify each amino acid present in the ABW05875 sequence.\\n\\n**Results:**\\n\\nTo perform this analysis, we utilized bioinformatics tools to extract the amino acid sequence of ABW05875 (accession number: ABW05875). The sequence was then subjected to a comprehensive amino acid composition analysis using a custom-written Python script. The results are presented in Table 1 below:\\n\\n| Amino Acid | Count |\\n| --- | --- |\\n| Alanine (A) | 12 |\\n| Arginine (R) | 8 |\\n| Asparagine (N) | 15 |\\n| Aspartic acid (D) | 10 |\\n| Cysteine (C) | 5 |\\n| Glutamic acid (E) | 18 |\\n| Glutamine (Q) | 12 |\\n| Glycine (G) | 20 |\\n| Histidine (H) | 6 |\\n| Isoleucine (I) | 8 |\\n| Leucine (L) | 15 |\\n| Lysine (K) | 10 |\\n| Methionine (M) | 5 |\\n| Phenylalanine (F) | 12 |\\n| Proline (P) | 8 |\\n| Serine (S) | 18 |\\n| Threonine (T) | 12 |\\n| Tryptophan (W) | 2 |\\n| Tyrosine (Y) | 6 |\\n| Valine (V) | 15 |\\n\\n**Discussion:**\\nThe amino acid composition of ABW05875 reveals a diverse range of residues, with Glycine (G) being the most abundant amino acid present in the sequence. This is consistent with the protein's putative enzymatic function, which may require flexibility and structural plasticity. The presence of charged residues such as Arginine (R), Glutamic acid (E), and Lysine (K) suggests that ABW05875 may interact with other proteins or molecules through electrostatic interactions.\\n\\n**Conclusion:**\\nIn conclusion, this study has provided a comprehensive analysis of the amino acid composition of the ABW05875 sequence. The results will be useful for further studies on the protein's structure-function relationships and potential enzymatic activities.\",\n",
       "  'Here is a scientific paper passage that answers the question:\\n\\n**Improving Context Retrieval through Step-Back Queries: A Case Study on Protein Sequence Analysis**\\n\\nThe process of analyzing protein sequences involves a range of computational and bioinformatics techniques, aimed at extracting meaningful information from the primary sequence. One key aspect of this analysis is the determination of amino acid composition, which can provide insights into the functional properties and structural characteristics of a protein (Bairoch & Apweiler, 2000). This process typically involves the use of specialized software tools, such as BLAST (Altschul et al., 1997) or MUSCLE (Edgar, 2004), which can identify similarities between query sequences and known proteins in databases.\\n\\nFurthermore, protein structure prediction is a critical component of sequence analysis, allowing researchers to infer the three-dimensional conformation of a protein from its primary sequence (Friedberg & Fainberg, 2013). This information can be used to predict functional sites, such as binding pockets or active sites, which are essential for understanding protein function and behavior. Additionally, bioinformatics tools and resources, such as UniProt (The UniProt Consortium, 2020) and Pfam (Finn et al., 2016), provide comprehensive databases and annotation systems that facilitate the analysis of protein sequences.\\n\\nBy considering these aspects of protein sequence analysis, researchers can gain a deeper understanding of the general process involved in analyzing protein sequences. This broader context can then be used to inform more specific queries, such as counting amino acids in a particular protein sequence, by providing relevant background information and computational tools for analysis.\\n\\nReferences:\\n\\nAltschul, S. F., Madden, T. L., Schäffer, A. A., Zhang, J., Zhang, Z., Miller, W., & Lipman, D. J. (1997). Gapped BLAST and PSI-BLAST: a new generation of protein database search programs. Nucleic Acids Research, 25(17), 3389-3402.\\n\\nBairoch, A., & Apweiler, R. (2000). The SWISS-PROT protein sequence database: current status. Nucleic Acids Research, 28(1), 45-48.\\n\\nEdgar, R. C. (2004). MUSCLE: multiple sequence alignment with high accuracy and high throughput. Nucleic Acids Research, 32(5), 1792-1797.\\n\\nFinn, R. D., Attwood, T. K., Bairoch, A., Barrell, B., Bateman, A., Binns, D., ... & Consortium, U. (2016). Pfam: the protein families database. Nucleic Acids Research, 44(D1), D279-D286.\\n\\nFriedberg, R., & Fainberg, H. (2013). Protein structure prediction: a review of methods and applications. Journal of Structural Biology, 183(2), 147-155.\\n\\nThe UniProt Consortium. (2020). UniProt: the universal protein knowledgebase. Nucleic Acids Research, 48(D1), D264-D268.',\n",
       "  'Here is a scientific paper passage answering the question:\\n\\n**Frequency Analysis of Amino Acids in Protein Sequence ABW05875**\\n\\nThe protein sequence ABW05875, a member of the ABC transporter family, was analyzed for its amino acid composition. Using bioinformatics tools and algorithms, we counted and listed the frequency of each individual amino acid present in this protein sequence.\\n\\nAs shown in Table 1, the most abundant amino acids in ABW05875 are alanine (ALA), serine (SER), and glycine (GLY), which together account for approximately 23.4% of the total amino acid residues. The least abundant amino acids are tryptophan (TRP) and tyrosine (TYR), with frequencies of only 1.2% and 2.5%, respectively.\\n\\n| Amino Acid | Frequency (%) |\\n| --- | --- |\\n| ALA | 9.8 |\\n| SER | 7.3 |\\n| GLY | 6.4 |\\n| THR | 5.6 |\\n| LEU | 5.1 |\\n| VAL | 4.8 |\\n| ILE | 4.2 |\\n| LYS | 3.9 |\\n| ARG | 3.5 |\\n| HIS | 3.2 |\\n| GLN | 2.9 |\\n| ASN | 2.7 |\\n| ASP | 2.5 |\\n| TYR | 2.5 |\\n| TRP | 1.2 |\\n| MET | 1.1 |\\n| CYS | 0.8 |\\n| PRO | 0.6 |\\n\\nThese results provide a detailed characterization of the amino acid composition of protein sequence ABW05875, which can be useful for predicting its structural and functional properties.\\n\\n**Table 1:** Frequency of each individual amino acid in protein sequence ABW05875.',\n",
       "  'Here is a scientific paper-style passage answering the question:\\n\\n**Title:** Frequency Analysis of Amino Acid Residues in the ABW05875 Sequence\\n\\n**Abstract:**\\nThe ABW05875 protein sequence, a member of the hypothetical protein family, has been analyzed to determine the frequency of its constituent amino acid residues. In this study, we report on the count of Alanine (A) residues within the ABW05875 sequence.\\n\\n**Results:**\\n\\nTo determine the frequency of Alanine (A) in the ABW05875 sequence, we performed a straightforward residue count. The ABW05875 sequence is composed of 123 amino acid residues. Upon examination, we found that the Alanine (A) residue occurs with a frequency of **23 times** within this sequence.\\n\\n**Discussion:**\\nThe high frequency of Alanine (A) in the ABW05875 sequence suggests that this protein may have evolved to optimize its structure and function for specific biological processes. Further analysis of the sequence, including secondary structure predictions and functional annotation, is warranted to fully understand the implications of this finding.\\n\\nNote: The actual count of 23 Alanine residues within the ABW05875 sequence would depend on the actual sequence provided (which was not given in your question). This passage assumes a hypothetical sequence for illustrative purposes.',\n",
       "  'Here is a scientific paper-style passage answering the question:\\n\\n**Title:** Frequency Analysis of Amino Acids in the ABW05875 Sequence\\n\\n**Abstract:**\\nThe ABW05875 protein sequence, a member of the hypothetical protein family, has been analyzed to determine its amino acid composition. In this study, we report on the frequency of cysteine (C) residues within the ABW05875 sequence.\\n\\n**Results:**\\n\\nTo determine the frequency of cysteine in the ABW05875 sequence, we counted the number of C residues present in the protein sequence. The ABW05875 sequence is as follows:\\n\\nMVLGSGSLLVTLAAGTSLALASLTVLGSGSLLVTLAAGTSLALASLTVLGSGSLLVTLAAGTSLALASLTV\\n\\nUsing a custom-written script, we counted the frequency of each amino acid in the sequence. The results are presented in Table 1.\\n\\n| Amino Acid | Frequency |\\n| --- | --- |\\n| Cysteine (C) | 5 |\\n\\n**Discussion:**\\nThe ABW05875 protein sequence contains a total of 5 cysteine residues, which represents a relatively low frequency compared to other amino acids. This result suggests that cysteine is not a dominant feature in the structure and function of this hypothetical protein.\\n\\n**Conclusion:**\\nIn conclusion, our analysis has shown that the ABW05875 sequence contains 5 cysteine residues, accounting for approximately 0.83% of the total amino acid composition. These findings provide insight into the biochemical properties of this hypothetical protein and may inform future studies on its structure and function.',\n",
       "  'Here is a scientific paper-style passage answering the question:\\n\\n**Title:** Frequency Analysis of Amino Acid Residues in the ABW05875 Protein Sequence\\n\\n**Abstract:**\\nThe ABW05875 protein sequence, a member of the [insert protein family], has been analyzed to determine the frequency of its constituent amino acid residues. In this study, we report on the count of Glycine (G) residues within the ABW05875 sequence.\\n\\n**Results:**\\n\\nTo determine the frequency of Glycine (G) in the ABW05875 sequence, we performed a simple residue count. The ABW05875 protein sequence is composed of 123 amino acid residues. Upon examination, we found that Glycine (G) was present at a frequency of 17 times within this sequence.\\n\\n**Discussion:**\\nThe observed frequency of Glycine (G) in the ABW05875 sequence is consistent with previous studies on protein sequences from [insert related organisms or protein families]. The high flexibility and conformational adaptability of Glycine residues may contribute to their relatively high frequency in certain protein sequences. Further analysis of the ABW05875 sequence and its functional implications are warranted.\\n\\n**Methods:**\\nThe ABW05875 protein sequence was retrieved from the [insert database or source]. Amino acid residue counts were performed using standard bioinformatics tools and software.\\n\\nNote: The actual count of Glycine (G) residues in the ABW05875 sequence is 17, as stated in the passage.',\n",
       "  'Here is a scientific paper-style passage that answers the question:\\n\\n**Title:** Amino Acid Composition Analysis of the ABW05875 Sequence\\n\\n**Abstract:**\\nThe ABW05875 protein sequence, a member of the hypothetical protein family, has been analyzed to determine its amino acid composition. In this study, we report the frequency of other amino acids (X) in the ABW05875 sequence.\\n\\n**Results:**\\n\\nTo determine the frequency of other amino acids (X) in the ABW05875 sequence, we employed a bioinformatics approach using the Sequence Analysis software package. The ABW05875 protein sequence was retrieved from the UniProt database and analyzed for its amino acid composition.\\n\\nThe results are presented in Table 1:\\n\\n| Amino Acid | Frequency |\\n| --- | --- |\\n| X (Other) | 23.4% |\\n| A | 12.5% |\\n| R | 8.2% |\\n| N | 6.3% |\\n| D | 5.9% |\\n| C | 4.7% |\\n| E | 4.1% |\\n| Q | 3.5% |\\n| G | 3.2% |\\n| H | 2.8% |\\n| I | 2.5% |\\n| L | 2.3% |\\n| K | 2.1% |\\n| M | 1.9% |\\n| F | 1.7% |\\n| P | 1.5% |\\n| S | 1.4% |\\n| T | 1.3% |\\n| W | 1.2% |\\n| Y | 1.1% |\\n\\n**Discussion:**\\nThe results show that other amino acids (X) account for approximately 23.4% of the ABW05875 sequence, indicating a significant presence of non-standard amino acids in this protein. The frequency of other amino acids is higher than expected, suggesting potential post-translational modifications or alternative splicing events.\\n\\n**Conclusion:**\\nIn conclusion, our analysis reveals that other amino acids (X) are present at a relatively high frequency in the ABW05875 sequence. Further studies are needed to elucidate the functional significance of these non-standard amino acids and their impact on protein structure and function.'],\n",
       " 'documents': [Document(metadata={}, page_content='Protein ID: 157955043\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD'),\n",
       "  Document(metadata={}, page_content='Protein ID: 157955043\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD'),\n",
       "  Document(metadata={}, page_content='Protein ID: 157955043\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD'),\n",
       "  Document(metadata={}, page_content='Protein ID: 157955043\\nType: protein\\nName: ABW05875\\nOrganism: Bromelia pinguin\\nDescription: PsbN (chloroplast) [Bromelia pinguin]\\nComment: Method: conceptual translation.\\nSequence: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD')],\n",
       " 'web_search': 'No',\n",
       " 'generation': 'The amino acid sequence for ABW05875 is: METATLVAISISGLLVSFTGYALYTAFGQPSQQLRDPFEEHGD. The count of each amino acid in this sequence is: M: 2, E: 2, T: 2, A: 2, L: 2, V: 1, I: 1, S: 1, G: 1, F: 1, Y: 1, Q: 1, P: 1, R: 1.',\n",
       " 'generations_num': 1}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "app.invoke({\"question\": 'Count each amino acid in the ABW05875 sequence'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the afferent cranial nerve nuclei?</td>\n",
       "      <td>Trigeminal sensory nucleus- fibres carry gener...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the order of the cranial nerves ?</td>\n",
       "      <td>1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the efferent cranial nerve nuclei?</td>\n",
       "      <td>Edinger-westphal nucleus\\nOculomotor nucleus\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which nuclei share the embryo logical origin -...</td>\n",
       "      <td>Oculomotor nucleus Trochlear nucleus Abducens ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which nuclei share the embryo logical origin- ...</td>\n",
       "      <td>Trigeminal motor nucleus Facial motor nucleus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>What is the purpose of gephyrin in the glycine...</td>\n",
       "      <td>Involved in anchoring the receptor to a specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>What is the glycine receptor involved in ?</td>\n",
       "      <td>Reflex response\\nCauses reciprocal inhibition ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>What happens in hyperperplexia ?</td>\n",
       "      <td>It’s an exaggerated reflex Often caused by a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>What is hyperperplexia treated with ?</td>\n",
       "      <td>Benzodiazepine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>What increases glycine release ?</td>\n",
       "      <td>Tetanus toxin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1052 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0           What are the afferent cranial nerve nuclei?   \n",
       "1             What is the order of the cranial nerves ?   \n",
       "2           What are the efferent cranial nerve nuclei?   \n",
       "3     Which nuclei share the embryo logical origin -...   \n",
       "4     Which nuclei share the embryo logical origin- ...   \n",
       "...                                                 ...   \n",
       "1047  What is the purpose of gephyrin in the glycine...   \n",
       "1048         What is the glycine receptor involved in ?   \n",
       "1049                   What happens in hyperperplexia ?   \n",
       "1050              What is hyperperplexia treated with ?   \n",
       "1051                   What increases glycine release ?   \n",
       "\n",
       "                                                 answer  \n",
       "0     Trigeminal sensory nucleus- fibres carry gener...  \n",
       "1     1-olfactory\\n2-optic\\n3-oculomotor\\n4-trochlea...  \n",
       "2     Edinger-westphal nucleus\\nOculomotor nucleus\\n...  \n",
       "3     Oculomotor nucleus Trochlear nucleus Abducens ...  \n",
       "4     Trigeminal motor nucleus Facial motor nucleus ...  \n",
       "...                                                 ...  \n",
       "1047  Involved in anchoring the receptor to a specif...  \n",
       "1048  Reflex response\\nCauses reciprocal inhibition ...  \n",
       "1049  It’s an exaggerated reflex Often caused by a m...  \n",
       "1050                                     Benzodiazepine  \n",
       "1051                                      Tetanus toxin  \n",
       "\n",
       "[1052 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.read_csv('brainscape.csv')\n",
    "qa_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cached RAGs responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_path = Path('cache.json')\n",
    "\n",
    "if not os.path.exists(cache_path):\n",
    "  data = {}\n",
    "  with open(cache_path, 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "with open(cache_path, 'r') as f:\n",
    "  cache = json.load(f)\n",
    "\n",
    "len(cache.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1052it [00:01, 981.70it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cache_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(cache, f)\n\u001b[0;32m---> 14\u001b[0m cos_score \u001b[38;5;241m=\u001b[39m \u001b[43membeddings_cosine_sim_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_answers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_answers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m bleu_metric(expected_answers, predicted_answers)\n\u001b[1;32m     16\u001b[0m rogue_1_score \u001b[38;5;241m=\u001b[39m rogue_1_metric(expected_answers, predicted_answers)\n",
      "Cell \u001b[0;32mIn[103], line 9\u001b[0m, in \u001b[0;36membeddings_cosine_sim_metric\u001b[0;34m(expected_answers, predicted_answers)\u001b[0m\n\u001b[1;32m      6\u001b[0m predicted_answer \u001b[38;5;241m=\u001b[39m preprocess(predicted_answer)\n\u001b[1;32m      8\u001b[0m expected_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(cached_embeddings\u001b[38;5;241m.\u001b[39membed_query(expected_answer))\n\u001b[0;32m----> 9\u001b[0m predicted_embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mcached_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_answer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     11\u001b[0m sim \u001b[38;5;241m=\u001b[39m cosine_similarity(\n\u001b[1;32m     12\u001b[0m   expected_embedding\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     13\u001b[0m   predicted_embedding\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     14\u001b[0m )[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     16\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(sim)\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/langchain/embeddings/cache.py:194\u001b[0m, in \u001b[0;36mCacheBackedEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed query text.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mBy default, this method does not cache queries. To enable caching, set the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    The embedding for the given text.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_embedding_store:\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munderlying_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m (cached,) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_embedding_store\u001b[38;5;241m.\u001b[39mmget([text])\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/langchain_community/embeddings/ollama.py:227\u001b[0m, in \u001b[0;36mOllamaEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed a query using a Ollama deployed embedding model.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    Embeddings for the text.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    226\u001b[0m instruction_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 227\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minstruction_pair\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/langchain_community/embeddings/ollama.py:202\u001b[0m, in \u001b[0;36mOllamaEmbeddings._embed\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_emb_response(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/langchain_community/embeddings/ollama.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_emb_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m iter_]\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/langchain_community/embeddings/ollama.py:167\u001b[0m, in \u001b[0;36mOllamaEmbeddings._process_emb_response\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[1;32m    164\u001b[0m }\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by inference endpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/requests/api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/biorag/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "questions = list(qa_df['question'].tolist())\n",
    "expected_answers = list(qa_df['answer'].tolist())\n",
    "predicted_answers = []\n",
    "\n",
    "for index, question in tqdm(enumerate(questions)):\n",
    "  if not question in cache:\n",
    "    cache[question] = app.invoke({'question': question})['generation']\n",
    "\n",
    "  predicted_answers.append(cache[question])\n",
    "\n",
    "  with open(cache_path, 'w') as f:\n",
    "    json.dump(cache, f)\n",
    "\n",
    "cos_score = embeddings_cosine_sim_metric(expected_answers, predicted_answers)\n",
    "bleu_score = bleu_metric(expected_answers, predicted_answers)\n",
    "rogue_1_score = rogue_1_metric(expected_answers, predicted_answers)\n",
    "rogue_l_score = rogue_l_metric(expected_answers, predicted_answers)\n",
    "\n",
    "cos_score, bleu_score, rogue_1_score, rogue_l_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
